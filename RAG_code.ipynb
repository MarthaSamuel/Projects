{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6def489-4487-4388-9915-b19f19f53276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution - (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tf-keras==2.19 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.19.0)\n",
      "Collecting tensorflow<2.20,>=2.19 (from tf-keras==2.19)\n",
      "  Using cached tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (3.4.0)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (1.70.0)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow<2.20,>=2.19->tf-keras==2.19)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: keras>=3.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras==2.19) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras==2.19) (0.45.1)\n",
      "Requirement already satisfied: rich in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras==2.19) (13.9.4)\n",
      "Requirement already satisfied: namex in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras==2.19) (0.0.8)\n",
      "Requirement already satisfied: optree in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras==2.19) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras==2.19) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras==2.19) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras==2.19) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras==2.19) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras==2.19) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras==2.19) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras==2.19) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras==2.19) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras==2.19) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras==2.19) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras==2.19) (0.1.2)\n",
      "Using cached tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.8 MB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tensorboard, tensorflow\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.18.0\n",
      "    Uninstalling tensorboard-2.18.0:\n",
      "      Successfully uninstalled tensorboard-2.18.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.18.1\n",
      "    Uninstalling tensorflow-2.18.1:\n",
      "      Successfully uninstalled tensorflow-2.18.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tensorboard-2.19.0 tensorflow-2.19.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.3.2)\n",
      "Requirement already satisfied: keras-nlp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.18.1)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: keras-hub==0.18.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras-nlp) (0.18.1)\n",
      "Requirement already satisfied: absl-py in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (2.1.0)\n",
      "Requirement already satisfied: rich in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (13.9.4)\n",
      "Requirement already satisfied: kagglehub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (0.3.12)\n",
      "Requirement already satisfied: tensorflow-text in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras-hub==0.18.1->keras-nlp) (2.18.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich->keras-hub==0.18.1->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich->keras-hub==0.18.1->keras-nlp) (2.19.1)\n",
      "Collecting tensorflow<2.19,>=2.18.0 (from tensorflow-text->keras-hub==0.18.1->keras-nlp)\n",
      "  Using cached tensorflow-2.18.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-hub==0.18.1->keras-nlp) (0.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (5.29.4)\n",
      "Requirement already satisfied: setuptools in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (75.8.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (1.70.0)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (3.10.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.45.1)\n",
      "Requirement already satisfied: namex in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.0.8)\n",
      "Requirement already satisfied: optree in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.14.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.18.1->keras-nlp) (2.1.5)\n",
      "Using cached tensorflow-2.18.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.4 MB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tensorboard, tensorflow\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.19.0\n",
      "    Uninstalling tensorboard-2.19.0:\n",
      "      Successfully uninstalled tensorboard-2.19.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.19.0\n",
      "    Uninstalling tensorflow-2.19.0:\n",
      "      Successfully uninstalled tensorflow-2.19.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tf-keras 2.19.0 requires tensorflow<2.20,>=2.19, but you have tensorflow 2.18.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tensorboard-2.18.0 tensorflow-2.18.1\n",
      "zsh:1: 3 not found\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: python-docx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.1.2)\n",
      "Requirement already satisfied: PyMuPDF in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.25.3)\n",
      "Requirement already satisfied: gradio in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (5.17.1)\n",
      "Requirement already satisfied: python-pptx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.0.2)\n",
      "Requirement already satisfied: pypdf in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (5.4.0)\n",
      "Requirement already satisfied: unstructured in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.17.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-docx) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (4.8.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.115.9)\n",
      "Requirement already satisfied: ffmpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (1.7.1)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.29.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (3.1.5)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (3.10.15)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (2.1.4)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (2.9.2)\n",
      "Requirement already satisfied: pydub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.9.7)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.45.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.15.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio) (0.34.0)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio-client==1.7.1->gradio) (2024.12.0)\n",
      "Requirement already satisfied: websockets<15.0,>=10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gradio-client==1.7.1->gradio) (14.2)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-pptx) (3.2.2)\n",
      "Requirement already satisfied: chardet in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured) (5.2.0)\n",
      "Requirement already satisfied: filetype in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: nltk in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured) (3.9.1)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured) (4.13.3)\n",
      "Requirement already satisfied: emoji in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured) (2.14.1)\n",
      "Requirement already satisfied: dataclasses-json in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured) (2025.2.18)\n",
      "Requirement already satisfied: langdetect in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured) (1.0.9)\n",
      "Requirement already satisfied: rapidfuzz in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured) (3.12.2)\n",
      "Requirement already satisfied: backoff in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: unstructured-client in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured) (0.27.0)\n",
      "Requirement already satisfied: wrapt in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured) (1.14.1)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured) (4.67.1)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured) (6.1.1)\n",
      "Requirement already satisfied: python-oxmsg in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured) (0.0.2)\n",
      "Requirement already satisfied: html5lib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured) (1.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from beautifulsoup4->unstructured) (2.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from dataclasses-json->unstructured) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six>=1.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from html5lib->unstructured) (1.17.0)\n",
      "Requirement already satisfied: webencodings in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from html5lib->unstructured) (0.5.1)\n",
      "Requirement already satisfied: joblib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->unstructured) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->unstructured) (2024.11.6)\n",
      "Requirement already satisfied: olefile in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-oxmsg->unstructured) (0.47)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->unstructured) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->unstructured) (2.3.0)\n",
      "Requirement already satisfied: cryptography>=3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured-client->unstructured) (44.0.2)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured-client->unstructured) (0.2.2)\n",
      "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured-client->unstructured) (1.0.6)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
      "Requirement already satisfied: pycparser in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.3.19)\n",
      "Requirement already satisfied: langchain_core in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.3.37)\n",
      "Requirement already satisfied: langchain-google-vertexai in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.0.13)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (0.3.10)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (2.0.38)\n",
      "Requirement already satisfied: requests<3,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (3.11.12)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain_core) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain_core) (4.12.2)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.76.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain-google-vertexai) (1.81.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0,>=2.18.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain-google-vertexai) (2.19.0)\n",
      "Requirement already satisfied: httpx<0.29.0,>=0.28.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain-google-vertexai) (0.28.1)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langchain-google-vertexai) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain-google-vertexai) (2.24.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.76.0->langchain-google-vertexai) (2.38.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.76.0->langchain-google-vertexai) (1.26.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.76.0->langchain-google-vertexai) (5.29.4)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.76.0->langchain-google-vertexai) (3.29.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.76.0->langchain-google-vertexai) (1.14.1)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.76.0->langchain-google-vertexai) (2.0.7)\n",
      "Requirement already satisfied: docstring-parser<1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.76.0->langchain-google-vertexai) (0.16)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain-google-vertexai) (2.4.2)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain-google-vertexai) (2.7.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain-google-vertexai) (1.6.0)\n",
      "Requirement already satisfied: anyio in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx<0.29.0,>=0.28.0->langchain-google-vertexai) (4.8.0)\n",
      "Requirement already satisfied: certifi in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx<0.29.0,>=0.28.0->langchain-google-vertexai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx<0.29.0,>=0.28.0->langchain-google-vertexai) (1.0.7)\n",
      "Requirement already satisfied: idna in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx<0.29.0,>=0.28.0->langchain-google-vertexai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.29.0,>=0.28.0->langchain-google-vertexai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain-google-vertexai) (1.68.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain-google-vertexai) (1.70.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain-google-vertexai) (1.70.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain-google-vertexai) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain-google-vertexai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain-google-vertexai) (4.9)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain-google-vertexai) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.14.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain-google-vertexai) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio->httpx<0.29.0,>=0.28.0->langchain-google-vertexai) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio->httpx<0.29.0,>=0.28.0->langchain-google-vertexai) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain-google-vertexai) (0.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil<3.0dev,>=2.7.3->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain-google-vertexai) (1.17.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#GW\n",
    "!pip install tf-keras==2.19\n",
    "#GW\n",
    "!pip install transformers datasets keras-nlp \n",
    "!pip install keras>=3 tensorflow-text huggingface-hub peft langchain_community chromadb sentence-transformers llama-index faiss-cpu #GW\n",
    "#!pip install keras tensorflow-text huggingface-hub peft langchain_community chromadb sentence-transformers llama-index faiss-cpu\n",
    "!pip install python-docx PyMuPDF gradio python-pptx  pypdf unstructured pymupdf\n",
    "!pip install langchain langchain_core langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d582f7-1b28-4bca-9955-5f1f40d78796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dda4aef-b61e-4a96-bf1e-5890c853cc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution - (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc66dd56-c17c-464f-b8a4-646c19c62f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c892d03-8a06-4cf0-bb2d-c3d1fd203679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 09:53:10.869171: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748166790.916044   29712 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748166790.928986   29712 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-25 09:53:11.029025: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import keras\n",
    "import keras_nlp\n",
    "import keras_hub\n",
    "from IPython.display import Markdown\n",
    "import textwrap\n",
    "import gradio as gr\n",
    "import langchain_core\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, UnstructuredWordDocumentLoader, UnstructuredPowerPointLoader\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import pymupdf\n",
    "import fitz \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from docx import Document\n",
    "from pptx import Presentation\n",
    "from langchain.schema import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c61e48-1126-4a0e-8793-7a50eb22bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuring notebook\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n",
    "# Avoid memory fragmentation on JAX backend.\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6fbfc2f-f49c-42de-bb72-4b888605e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining fn to display prompt and response prettily\n",
    "\n",
    "def display_chat(prompt, response):\n",
    "  '''Displays an LLM prompt and response in a pretty way.'''\n",
    "  prompt = prompt.replace('\\n\\n','')\n",
    "  prompt = prompt.replace('\\n','')\n",
    "  formatted_prompt = \"\" + prompt + \"\"\n",
    "  response = response.replace('', '  *')\n",
    "  response = textwrap.indent(response, '', predicate=lambda _: True)\n",
    "  response = response.replace('\\n\\n','')\n",
    "  response = response.replace('\\n','')\n",
    "  response = response.replace(\"```\",\"\")\n",
    "  formatted_text = \"\" + response + \"\"\n",
    "  return Markdown(formatted_prompt+formatted_text)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c1bacf-1d04-4157-8652-f1c9a9e3fd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370d4da2d25346028ee92a10355b14c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "kagglehub.login()\n",
    "#38c08016f7eb7512a1352518ea800966"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98432dc9-d487-4a93-ad18-71a9c47962f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 09:54:00.658338: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-05-25 09:54:00.696721: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2359296000 exceeds 10% of free system memory.\n",
      "2025-05-25 09:54:04.994529: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2359296000 exceeds 10% of free system memory.\n",
      "2025-05-25 09:54:05.682077: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2359296000 exceeds 10% of free system memory.\n",
      "2025-05-25 09:54:29.194172: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2359296000 exceeds 10% of free system memory.\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                                                  </span><span style=\"font-weight: bold\">                                   Config </span>\n",
       "\n",
       " gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                                                    Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                                                    Vocab size: \u001b[38;5;34m256,000\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                  </span><span style=\"font-weight: bold\"> Output Shape              </span><span style=\"font-weight: bold\">         Param # </span><span style=\"font-weight: bold\"> Connected to               </span>\n",
       "\n",
       " padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n",
       "\n",
       " token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n",
       "\n",
       " gemma_backbone                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span>  padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)                                                            token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "\n",
       " token_embedding                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span>  gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)                                                                                 \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n",
       "\n",
       " token_ids (\u001b[38;5;33mInputLayer\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n",
       "\n",
       " gemma_backbone                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)           \u001b[38;5;34m2,614,341,888\u001b[0m  padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        \n",
       " (\u001b[38;5;33mGemmaBackbone\u001b[0m)                                                            token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "\n",
       " token_embedding                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)           \u001b[38;5;34m589,824,000\u001b[0m  gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       " (\u001b[38;5;33mReversibleEmbedding\u001b[0m)                                                                                 \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load model of choice\n",
    "\n",
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_instruct_2b_en\")\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc98cef-f3c3-487f-9be9-5ecf2004f3f4",
   "metadata": {},
   "source": [
    "load documents --> chunk --> embedding into vector space--> indexing vector space--> query + retrieval from a vector space --> top p matches or contexts + LLM --> prompt template --> response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6bd1f5a-a6c2-40d7-89bc-48e9f06ac815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pdfs..\n",
      "Load metadata...\n",
      "Load docx...\n",
      "Load docx_docs...\n",
      "loading pptx...\n"
     ]
    }
   ],
   "source": [
    "#function for loading documents\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader, UnstructuredWordDocumentLoader, UnstructuredPowerPointLoader\n",
    "\n",
    "# Load PDFs\n",
    "print(\"Load pdfs..\")\n",
    "#pdf_loader = DirectoryLoader(\"./././RAG_pdf\", glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "pdf_loader = DirectoryLoader(\"./././RAG_pdf\", glob=\"**/Contact.pdf\", loader_cls=PyPDFLoader)\n",
    "pdf_docs = pdf_loader.load()\n",
    "\n",
    "# Add metadata for PDFs which is optional\n",
    "print(\"Load metadata...\")\n",
    "for doc in pdf_docs:\n",
    "    doc.metadata = {\"source\": \"RAG_pdf\", \"file_name\": doc.metadata[\"source\"]}\n",
    "\n",
    "# Load DOCX\n",
    "print(\"Load docx...\")\n",
    "docx_loader = DirectoryLoader('./././RAG_docx', glob=\"**/*.docx\", loader_cls=UnstructuredWordDocumentLoader)\n",
    "docx_docs = docx_loader.load()\n",
    "\n",
    "# Add metadata for DOCX\n",
    "print(\"Load docx_docs...\")\n",
    "for doc in docx_docs:\n",
    "    doc.metadata = {\"source\": \"RAG_docx\", \"file_name\": doc.metadata[\"source\"]}\n",
    "\n",
    "# Load PPTX\n",
    "print(\"loading pptx...\")\n",
    "#GW pptx_loader = DirectoryLoader('./././RAG_pptx', glob=\"**/*.pptx\", loader_cls=UnstructuredPowerPointLoader)\n",
    "pptx_loader = DirectoryLoader('./././RAG_pptx', glob=\"**/ISMP*.pptx\", loader_cls=UnstructuredPowerPointLoader)\n",
    "pptx_docs = pptx_loader.load()\n",
    "\n",
    "\n",
    "# Add metadata for PPTX\n",
    "for doc in pptx_docs:\n",
    "    doc.metadata = {\"source\": \"RAG_pptx\", \"file_name\": doc.metadata[\"source\"]}\n",
    "\n",
    "\n",
    "# Combine all documents\n",
    "all_docs = pdf_docs + docx_docs + pptx_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f365834d-6cc5-4f67-9ce7-47254095aad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Advising Services _ Portland State University.docx',\n",
       " 'FAQ for International Student Mentors 2024.docx',\n",
       " 'OISSS and Immigration Scenarios.docx',\n",
       " 'On Campus Childcare _ Portland State University.docx']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.listdir('/teamspace/studios/this_studio/RAG_docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b52bdeb-02a0-49e1-865b-1ec7fa2d6165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For LangChain documents (metadata)\n",
    "#from langchain.docstore.document import Document as LangDocument\n",
    "\n",
    "# For Word document processing\n",
    "from docx import Document as DocxDocument\n",
    "\n",
    "# Function to create documents with metadata (source folder). this is optional\n",
    "def create_documents_with_metadata(texts, source_folder):\n",
    "    documents = []\n",
    "    for text in texts:\n",
    "        doc = Document(page_content=text, metadata={\"source_folder\": source_folder})\n",
    "        documents.append(doc)\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1241776-49ff-43ab-941a-955ecc35ed5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 970 documents with metadata\n"
     ]
    }
   ],
   "source": [
    "# alt functions for Loading document content from a PDF\n",
    "def load_pdfs(folder_path):\n",
    "    texts = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            pdf = fitz.open(file_path) \n",
    "            \n",
    "            # Extract text from each page\n",
    "            for page_num in range(len(pdf)):\n",
    "                page = pdf.load_page(page_num)\n",
    "                text = page.get_text()\n",
    "                texts.append(text)\n",
    "            \n",
    "            pdf.close()  # Close after processing each file\n",
    "    return texts\n",
    "        \n",
    "    \n",
    "\n",
    "def load_docx(folder_path):\n",
    "    texts = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".docx\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            doc = DocxDocument(file_path)  # Open the Word document\n",
    "            \n",
    "            # Extract text from each paragraph\n",
    "            for paragraph in doc.paragraphs:\n",
    "                if paragraph.text.strip():\n",
    "                    texts.append(paragraph.text)\n",
    "    return texts\n",
    "\n",
    "\n",
    "\n",
    "def load_pptx(folder_path):\n",
    "    texts = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".pptx\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            presentation = Presentation(file_path)  # Open the PPT file\n",
    "            \n",
    "            # Extract text from each slide\n",
    "            for slide in presentation.slides:\n",
    "                for shape in slide.shapes:\n",
    "                    if hasattr(shape, \"text\"):\n",
    "                        texts.append(shape.text)\n",
    "    return texts\n",
    "\n",
    "\n",
    "def load_all(folder_paths):\n",
    "    pdf_texts = load_pdfs(folder_paths['pdf'])\n",
    "    docx_texts = load_docx(folder_paths['docx'])\n",
    "    pptx_texts = load_pptx(folder_paths['pptx'])\n",
    "\n",
    "\n",
    "     #Create documents with metadata (source folder) for each type of file\n",
    "    pdf_docs = create_documents_with_metadata(pdf_texts, \"pdf\")\n",
    "    docx_docs = create_documents_with_metadata(docx_texts, \"docx\")\n",
    "    pptx_docs = create_documents_with_metadata(pptx_texts, \"pptx\")\n",
    "\n",
    "\n",
    "    # Combine all text into one list\n",
    "    all_texts = pdf_texts + docx_texts + pptx_texts\n",
    "    return all_texts\n",
    "\n",
    "\n",
    "\n",
    "folder_paths = {\n",
    "    'pdf': './././RAG_pdf',\n",
    "    'docx': './././RAG_docx',\n",
    "    'pptx': './././RAG_pptx'\n",
    "}\n",
    "\n",
    "all_texts = load_all(folder_paths)\n",
    "print(f\"Loaded {len(all_texts)} documents with metadata\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c87aa55b-cc7c-4374-9dd0-5357b96b292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace(\"\\n\", \" \").strip()  # Remove newlines\n",
    "    text = \" \".join(text.split())  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "for doc in all_docs:\n",
    "\n",
    "    doc.page_content = clean_text(doc.page_content)\n",
    "    #print(doc.page_content[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5b9c8d2-bcaa-4c49-81f4-3b086c818003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alt fn\n",
    "'''\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\n\", \" \").strip()  # Remove newlines\n",
    "    text = \" \".join(text.split())  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Clean all items in all_texts list\n",
    "all_texts = [clean_text(text) for text in all_texts]\n",
    "'''\n",
    "\n",
    "#making this compatible to langchain's pipeline, so we can add metadata e.g source, filename etc easily\n",
    "\n",
    "source_docs = [Document(page_content=text) for text in all_texts] #we dont need this line with the create_documents_with_metadata fn above \n",
    "\n",
    "# Clean text content within Document objects\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\n\", \" \").strip()  # Remove newlines\n",
    "    text = \" \".join(text.split())  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Clean each document\n",
    "for sdoc in source_docs:\n",
    "    sdoc.page_content = clean_text(sdoc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd803589-0da4-4705-91cc-c85a2bf2c21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain.text_splitter import CharacterTextSplitter\\n\\n# Create the text splitter\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\\n\\n# Split the documents\\nsplit_docs = []\\nfor doc in all_docs:\\n    split_docs.extend(text_splitter.split_documents([doc]))\\n\\n# Now, split_docs contains chunks of text from the original documents\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split Documents into Chunks\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Adjust based on your needs\n",
    "    chunk_overlap=250\n",
    ")\n",
    "chunks = text_splitter.split_documents(all_docs)#put source_docs\n",
    "\n",
    "# Print out chunked documents (optional for checking)\n",
    "# for chunk in chunks:\n",
    "    #print(chunk.page_content)\n",
    "\n",
    "\n",
    "'''\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Create the text splitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# Split the documents\n",
    "split_docs = []\n",
    "for doc in all_docs:\n",
    "    split_docs.extend(text_splitter.split_documents([doc]))\n",
    "\n",
    "# Now, split_docs contains chunks of text from the original documents\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f0d0b-4b54-4af0-8948-a55d3247e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#will troubleshoot later if i ever want to use hub.load...\n",
    "! pip install tensorflow==2.15.0 #GW\n",
    "! pip install tensorflow-hub \n",
    "\n",
    "#GW\n",
    "import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "#GW import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)  # Should be 2.15.0\n",
    "#print(\"TensorFlow Hub version:\", hub.__version__)  # Should be 0.16.1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d96f5197-94f6-498a-b890-99e39771d8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution - (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: chromadb in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.0.5)\n",
      "Requirement already satisfied: build>=1.0.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (2.9.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi==0.115.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (0.115.9)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (3.25.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (1.21.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (0.21.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (1.70.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (0.15.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (9.0.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (3.10.15)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
      "Requirement already satisfied: packaging>=19.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
      "Requirement already satisfied: anyio in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (4.8.0)\n",
      "Requirement already satisfied: certifi in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=4.19.0->chromadb) (0.22.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.68.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.32.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.53b1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.53b1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
      "Requirement already satisfied: asgiref~=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.23.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.29.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.12.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#GW\n",
    "! pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5554ea1f-11ad-4949-97f0-ef266f1fd1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GW \n",
    "''' to trblsht \n",
    "import tensorflow_hub as hub\n",
    "#GW\n",
    "\n",
    "model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "'''\n",
    "\n",
    "#GW\n",
    "'''\n",
    "def embed_fn(texts):\n",
    "    return model(texts).numpy().tolist()\n",
    "\n",
    "# Create a Chroma vector database from the documents\n",
    "# Important: Make sure to delete previous db (if any) or else retrieval returns lots of duplicates :)\n",
    "try:\n",
    "  db.delete_collection()\n",
    "except:\n",
    "  pass\n",
    "db = Chroma.from_documents(chunks, embed_fn)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "33663a14-6de2-429c-9d60-c7fa80164c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GW from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "#create embedding from documents \n",
    "\n",
    "# Load the embeddings model\n",
    "# NOTE: You might need to experiment with different models\n",
    "embeddings = HuggingFaceEmbeddings(model_name= 'all-MiniLM-L6-v2') #GW SentenceTransformer('all-MiniLM-L6-v2'))\n",
    "\n",
    "# Create a Chroma vector database from the documents\n",
    "# Important: Make sure to delete previous db (if any) or else retrieval returns lots of duplicates :)\n",
    "try:\n",
    "  db.delete_collection()\n",
    "except:\n",
    "  pass\n",
    "# Create Chroma vector store from chunks and save vectore store too\n",
    "db = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_db\")\n",
    "db.persist()  # Save for later reuse\n",
    "\n",
    "#to load from saved vector store\n",
    "# Load Chroma index\n",
    "#chroma_store = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "047a9ced-8068-4fab-b553-6c8168e73a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#the chroma is a better option for our use case\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.vectorstores import FAISS\\n\\n# Initialize the OpenAI Embeddings\\nembeddings = OpenAIEmbeddings()\\n\\n# Store embeddings in FAISS directly from split_docs\\nvector_store = FAISS.from_documents(chunks, embeddings)\\nvector_store.save_local(\"faiss_index\")  # Save for later reuse\\n# to Load FAISS index from the vector store\\nfaiss_store = FAISS.load_local(\"faiss_index\", embeddings)\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a retriever from the vector database\n",
    "# NOTE: You might need to experiment with retrieval parameters\n",
    "#retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"k\": 2, \"score_threshold\": 0.3})\n",
    "retriever = db.as_retriever(search_type=\"similarity\",search_kwargs={\"k\": 3} ) # Return top 5 results)\n",
    "'''\n",
    "#the chroma is a better option for our use case\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Initialize the OpenAI Embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Store embeddings in FAISS directly from split_docs\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "vector_store.save_local(\"faiss_index\")  # Save for later reuse\n",
    "# to Load FAISS index from the vector store\n",
    "faiss_store = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f925444-2805-434d-bd2f-358863d8264c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#optionally, we can add a reranker, esp when wer are working with multistep reasoning, and in this case increase k to retrieve more chunks\\n\\nfrom sentence_transformers import CrossEncoder\\n\\n# Load a cross-encoder model for reranking\\nreranker_model = CrossEncoder(\\'cross-encoder/ms-marco-MiniLM-L-12-v2\\')\\n\\n# Input: Query and list of documents\\nquery = \"Your query\"\\ndocuments = [\"Doc 1\", \"Doc 2\", \"Doc 3\"]\\n\\n# Score each document\\nscores = reranker_model.predict([(query, doc) for doc in documents])\\n\\n# Sort documents by score\\nranked_documents = [doc for _, doc in sorted(zip(scores, documents), reverse=True)]\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#optionally, we can add a reranker, esp when wer are working with multistep reasoning, and in this case increase k to retrieve more chunks\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load a cross-encoder model for reranking\n",
    "reranker_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')\n",
    "\n",
    "# Input: Query and list of documents\n",
    "query = \"Your query\"\n",
    "documents = [\"Doc 1\", \"Doc 2\", \"Doc 3\"]\n",
    "\n",
    "# Score each document\n",
    "scores = reranker_model.predict([(query, doc) for doc in documents])\n",
    "\n",
    "# Sort documents by score\n",
    "ranked_documents = [doc for _, doc in sorted(zip(scores, documents), reverse=True)]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "18a94dc7-00fc-46f1-863c-f3bc2af828bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to retain memory of chat...this is optional as well.turn-based chat management.\n",
    "\n",
    "class ChatState():\n",
    "  \"\"\"\n",
    "  Manages the conversation history for a turn-based chatbot\n",
    "  Follows the turn-based conversation guidelines for the Gemma family of models\n",
    "  documented at https://ai.google.dev/gemma/docs/formatting\n",
    "  \"\"\"\n",
    "\n",
    "  __START_TURN_USER__ = \"user\\n\"\n",
    "  __START_TURN_MODEL__ = \"model\\n\"\n",
    "  __END_TURN__ = \"\\n\"\n",
    "\n",
    "  def __init__(self, model, system=\"\"):\n",
    "    \"\"\"\n",
    "    Initializes the chat state.\n",
    "\n",
    "    Args:\n",
    "        model: The language model to use for generating responses.\n",
    "        system: (Optional) System instructions or bot description.\n",
    "    \"\"\"\n",
    "    self.model = model\n",
    "    self.system = system\n",
    "    self.history = []\n",
    "\n",
    "  def add_to_history_as_user(self, message):\n",
    "      \"\"\"\n",
    "      Adds a user message to the history with start/end turn markers.\n",
    "      \"\"\"\n",
    "      self.history.append(self.__START_TURN_USER__ + message + self.__END_TURN__)\n",
    "\n",
    "  def add_to_history_as_model(self, message):\n",
    "      \"\"\"\n",
    "      Adds a model response to the history with start/end turn markers.\n",
    "      \"\"\"\n",
    "      self.history.append(self.__START_TURN_MODEL__ + message + self.__END_TURN__)\n",
    "\n",
    "  def get_history(self):\n",
    "      \"\"\"\n",
    "      Returns the entire chat history as a single string.\n",
    "      \"\"\"\n",
    "      return \"\".join([*self.history])\n",
    "\n",
    "  def get_full_prompt(self):\n",
    "    \"\"\"\n",
    "    Builds the prompt for the language model, including history and system description.\n",
    "    \"\"\"\n",
    "    prompt = self.get_history() + self.__START_TURN_MODEL__\n",
    "    if len(self.system)>0:\n",
    "      prompt = self.system + \"\\n\" + prompt\n",
    "    return prompt\n",
    "\n",
    "  def send_message(self, message):\n",
    "    \"\"\"\n",
    "    Handles sending a user message and getting a model response.\n",
    "\n",
    "    Args:\n",
    "        message: The user's message.\n",
    "\n",
    "    Returns:\n",
    "        The model's response.\n",
    "    \"\"\"\n",
    "    self.add_to_history_as_user(message)\n",
    "    prompt = self.get_full_prompt()\n",
    "    response = self.model.generate(prompt, max_length=1024)\n",
    "    result = response.replace(prompt, \"\")  # Extract only the new response\n",
    "    self.add_to_history_as_model(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493b9935-cc6f-43f7-863b-ccd98c02f5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4ed3ffbc-d841-4714-a42b-7aeb8e1a4a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retreived 3 documents\n",
      "[Document(metadata={'source': 'RAG_docx', 'file_name': 'RAG_docx/Advising Services _ Portland State University.docx'}, page_content='email intl-sponsor@pdx.edu. You can meet with any International Student advisor for questions about your F-1 or J-1 status. Visiting Scholars: Visiting scholars, faculty, and researchers on the J-1 visa should make an appointment with David Brandt. New and prospective students: If you have questions about applying to PSU, transcripts, the English language requirement, or other admissions questions, contact the International Admissions office. 2/9 MAKE AN APPOINTMENT To see all available appointment times (not advisor specific): 1. Follow the link for International Student Advisors or Student Life Advisors 2. On the advisor list page, scroll to the bottom to choose \"No Preference.\" 3. Select a time from the calendar. 4. Enter your details and reserve the appointment. Meet an International Student Advisor: https://immigration-advisors.youcanbook.me/ Meet a Student Life Advisor: https://student-life-advisors.youcanbook.me/ To meet with a particular advisor, use the profiles below.'), Document(metadata={'file_name': 'RAG_docx/Advising Services _ Portland State University.docx', 'source': 'RAG_docx'}, page_content='International Students ADVISING SERVICES IKARL MILLER CENTER & State FRONT DESK SERVICES For general information and quick questions, visit our front desk services. A staff or student member can chat with you and provide information, resources, and help schedule an appointment with an International Student Advisor. Virtual Front Desk Hours Monday - Friday 11:00am - 1:00pm Access the Virtual Front Desk In-person Front Desk Hours (Summer) Karl Miller Center, Suite 660 Monday to Friday from 9:00am - 5:00pm University Center Building, Suite 400 Monday to Friday from 9:00am - 3:00pm 1/9 STOPPING OR PARKING EXIT OPEN A VISIT THE VIRTUAL FRONT DESK >> 323 C DOLL WHO IS MY ADVISOR? Students with F-1 visa: We do not have assigned advisors, but we divide document review requests based on PSU ID so that requests can be processed consistently for all students. You can meet with any International Student Advisor whose availability fits your schedule, but we encourage you to meet with the advisor'), Document(metadata={'source': 'RAG_docx', 'file_name': 'RAG_docx/Advising Services _ Portland State University.docx'}, page_content='divide document review requests based on PSU ID so that requests can be processed consistently for all students. You can meet with any International Student Advisor whose availability fits your schedule, but we encourage you to meet with the advisor who will be working on your request if possible. Last digit of PSU ID and advisor: 0,7 - Christine Igarta 1, 4, 8 - Jonna Lynn Bransford 2, 3, 9 - Csendi Hopp 5, 6 - Joshua Davis New transfer students: Meet with Joshua Davis Students with J-1 visa: You can meet with any International Student advisor for questions about your J-1 status. For specific questions about your exchange program, see the advisor who manages your exchange program. Sponsored Students: For questions about your scholarship, email intl-sponsor@pdx.edu. You can meet with any International Student advisor for questions about your F-1 or J-1 status. Visiting Scholars: Visiting scholars, faculty, and researchers on the J-1 visa should make an appointment with David Brandt.')]\n",
      "Result 1:\n",
      "email intl-sponsor@pdx.edu. You can meet with any International Student advisor for questions about your F-1 or J-1 status. Visiting Scholars: Visiting scholars, faculty, and researchers on the J-1 visa should make an appointment with David Brandt. New and prospective students: If you have questions about applying to PSU, transcripts, the English language requirement, or other admissions questions, contact the International Admissions office. 2/9 MAKE AN APPOINTMENT To see all available appointment times (not advisor specific): 1. Follow the link for International Student Advisors or Student Life Advisors 2. On the advisor list page, scroll to the bottom to choose \"No Preference.\" 3. Select a time from the calendar. 4. Enter your details and reserve the appointment. Meet an International Student Advisor: https://immigration-advisors.youcanbook.me/ Meet a Student Life Advisor: https://student-life-advisors.youcanbook.me/ To meet with a particular advisor, use the profiles below.\n",
      "\n",
      "Result 2:\n",
      "International Students ADVISING SERVICES IKARL MILLER CENTER & State FRONT DESK SERVICES For general information and quick questions, visit our front desk services. A staff or student member can chat with you and provide information, resources, and help schedule an appointment with an International Student Advisor. Virtual Front Desk Hours Monday - Friday 11:00am - 1:00pm Access the Virtual Front Desk In-person Front Desk Hours (Summer) Karl Miller Center, Suite 660 Monday to Friday from 9:00am - 5:00pm University Center Building, Suite 400 Monday to Friday from 9:00am - 3:00pm 1/9 STOPPING OR PARKING EXIT OPEN A VISIT THE VIRTUAL FRONT DESK >> 323 C DOLL WHO IS MY ADVISOR? Students with F-1 visa: We do not have assigned advisors, but we divide document review requests based on PSU ID so that requests can be processed consistently for all students. You can meet with any International Student Advisor whose availability fits your schedule, but we encourage you to meet with the advisor\n",
      "\n",
      "Result 3:\n",
      "divide document review requests based on PSU ID so that requests can be processed consistently for all students. You can meet with any International Student Advisor whose availability fits your schedule, but we encourage you to meet with the advisor who will be working on your request if possible. Last digit of PSU ID and advisor: 0,7 - Christine Igarta 1, 4, 8 - Jonna Lynn Bransford 2, 3, 9 - Csendi Hopp 5, 6 - Joshua Davis New transfer students: Meet with Joshua Davis Students with J-1 visa: You can meet with any International Student advisor for questions about your J-1 status. For specific questions about your exchange program, see the advisor who manages your exchange program. Sponsored Students: For questions about your scholarship, email intl-sponsor@pdx.edu. You can meet with any International Student advisor for questions about your F-1 or J-1 status. Visiting Scholars: Visiting scholars, faculty, and researchers on the J-1 visa should make an appointment with David Brandt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing the rtriever on prompts. we use the list of returned documents as context\n",
    "\n",
    "ret_docs = retriever.invoke(\"how do i book appointment with an advisor\")\n",
    "print(\"retreived\", len(ret_docs), \"documents\")\n",
    "print(ret_docs)\n",
    "\n",
    "\n",
    "query = \"how do i book appointment with an advisor\"\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i + 1}:\\n{result.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "19b9eb7f-2c7a-4a8f-8383-d59a3862545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a prompt template\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Create a prompt template for RAG\n",
    "template = \"\"\"You are a helpful assistant. Use the following context to answer the question accurately.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: \n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b952279-470d-4438-8dec-7128b44da5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution - (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mName: torch\n",
      "Version: 2.2.1+cu121\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-nccl-cu12, nvidia-nvtx-cu12, sympy, triton, typing-extensions\n",
      "Required-by: accelerate, lightning, litdata, peft, pytorch-lightning, sentence-transformers, torchmetrics, torchvision\n"
     ]
    }
   ],
   "source": [
    "!pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "441b56a4-7d39-4ce8-aa2e-2d95d96e86d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nor use\\n\\nmodel_name = \"google/flan-t5-large\"\\n\\n or use \\n\\n \\nfrom langchain.llms import HuggingFacePipeline\\nimport torch\\n\\nmodel_name = \"meta-llama/Llama-2-7b-chat-hf\"\\nllm = HuggingFacePipeline.from_model_id(\\n    model_id=model_name,\\n    task=\"text-generation\",\\n    device_map=\"auto\",\\n    model_kwargs={\"torch_dtype\": torch.float16}\\n)\\n\\n#or say\\n\\nfrom langchain.chat_models import ChatGoogleGenerativeAI\\n\\n# Initialize Gemini model\\nllm = ChatGoogleGenerativeAI(model=\"gemini-pro\") \\n\\nfrom langchain_google_vertexai import VertexAI  # <button class=\"citation-flag\" data-index=\"7\">\\n\\nllm = VertexAI(model_name=\"text-bison@001\")  # Or another Vertex AI model\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the RAG  RetrievalQA chain using huggong face automodel and pipeline. we could use the kerasnlp model already downloaded above\n",
    "from transformers import pipeline, GenerationConfig, AutoTokenizer,  AutoModelForSeq2SeqLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "\n",
    "# Load the FLAN-T5-Large model and tokenizer\n",
    "model_name = \"google/flan-t5-large\" # we can use larger model to avoid chunking or truncation during genration\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "text_generator = pipeline(\n",
    "    \"text2text-generation\",  # Use \"text2text-generation\" for flexibility\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,  # Maximum length of the generated text,\n",
    "    min_length=30,   # Minimum length of the generated text\n",
    "    do_sample=True,  # Enable sampling for diverse outputs\n",
    "    temperature=0.7, # Control randomness (lower = more deterministic)\n",
    "    top_k=50,        # Limit sampling to the top-k tokens\n",
    "    top_p=0.95       # Nucleus sampling (cumulative probability threshold)\n",
    ")\n",
    "\n",
    "# Wrap with LangChain\n",
    "llm = HuggingFacePipeline(pipeline=text_generator)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "or use\n",
    "\n",
    "model_name = \"google/flan-t5-large\"\n",
    "\n",
    " or use \n",
    "\n",
    " \n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_name,\n",
    "    task=\"text-generation\",\n",
    "    device_map=\"auto\",\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")\n",
    "\n",
    "#or say\n",
    "\n",
    "from langchain.chat_models import ChatGoogleGenerativeAI\n",
    "\n",
    "# Initialize Gemini model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\") \n",
    "\n",
    "from langchain_google_vertexai import VertexAI  # <button class=\"citation-flag\" data-index=\"7\">\n",
    "\n",
    "llm = VertexAI(model_name=\"text-bison@001\")  # Or another Vertex AI model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9227c4e-36c6-44cc-af32-1221b57387c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChatState if in use\n",
    "chat_state = ChatState(model=llm, system=\"You are an expert assistant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7f6fa74a-d92e-43ee-9b7b-ef2552bf012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever, #reranker\n",
    "    input_key=\"query\",\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "97ab4696-7e21-4f2f-b4a7-313ece812af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do I book an appointment with an advisor?\"\n",
    "\n",
    "if not query.strip():\n",
    "    raise ValueError(\"Query cannot be empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e19d4326-b7b4-474b-9ea9-4cde77fa54a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 relevant documents.\n"
     ]
    }
   ],
   "source": [
    "retrieved_documents = retriever.get_relevant_documents(query)\n",
    "\n",
    "if not retrieved_documents:\n",
    "    print(\"No relevant documents found for the query.\")\n",
    "else:\n",
    "    print(f\"Found {len(retrieved_documents)} relevant documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "041110ea-b948-495d-a533-710ab680ff65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Output:\n",
      "{'input_ids': tensor([[ 571,  103,   27,  484,   46, 4141,   28,   46, 8815,   58,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "Decoded Text:\n",
      "How do I book an appointment with an advisor?\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"How do I book an appointment with an advisor?\"\n",
    "\n",
    "# Tokenize the query\n",
    "tokenized_output = tokenizer(query, return_tensors=\"pt\")  # \"pt\" for PyTorch tensors\n",
    "\n",
    "# Print the tokenized output\n",
    "print(\"Tokenized Output:\")\n",
    "print(tokenized_output)\n",
    "\n",
    "# Decode the token IDs back to text\n",
    "decoded_text = tokenizer.decode(tokenized_output[\"input_ids\"][0], skip_special_tokens=True)\n",
    "print(\"\\nDecoded Text:\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5aa170c8-6a57-4a39-8024-6c1e9ed3f52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 384\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedding_model.embed_query(query)\n",
    "\n",
    "print(f\"Embedding size: {len(embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "71016f6e-145a-40c0-963f-1fa5cd4788c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " 1. Follow the link for International Student Advisors or Student Life Advisors 2. On the advisor list page, scroll to the bottom to choose \"No Preference.\" 3. Select a time from the calendar 4. Enter your details and reserve the appointment.\n",
      "\n",
      "Sources:\n",
      "- email intl-sponsor@pdx.edu. You can meet with any International Student advisor for questions about your F-1 or J-1 status. Visiting Scholars: Visiting scholars, faculty, and researchers on the J-1 vi...\n",
      "- International Students ADVISING SERVICES IKARL MILLER CENTER & State FRONT DESK SERVICES For general information and quick questions, visit our front desk services. A staff or student member can chat ...\n",
      "- divide document review requests based on PSU ID so that requests can be processed consistently for all students. You can meet with any International Student Advisor whose availability fits your schedu...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nif we have the error: Token indices sequence length is longer than the specified maximum sequence length for this model (1255 > 512). Running this sequence through the model will result in indexing errors\\n\\nwe can tokenize query with truncation:\\nexample:\\nquery = \"How do I book an appointment with an advisor?\"\\n\\n# Tokenize with truncation\\ntokenized_output = tokenizer(\\n    query,\\n    return_tensors=\"pt\",\\n    max_length=512,        # Truncate input to fit within the model\\'s limit\\n    truncation=True        # Automatically truncate if input is too long\\n)\\n\\n# Decode truncated input\\ntruncated_query = tokenizer.decode(tokenized_output[\"input_ids\"][0], skip_special_tokens=True)\\nprint(\"\\nTruncated Query:\")\\nprint(truncated_query)\\n\\n# Generate response\\nresponse = text_generator(truncated_query)\\nprint(\"\\nGenerated Response:\")\\nprint(response[0][\\'generated_text\\'])\\n'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"how do i book appointment with an advisor?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Output the answer\n",
    "print(\"\\nAnswer:\\n\", result[\"result\"])\n",
    "\n",
    "# (Optional) Show which chunks were used\n",
    "print(\"\\nSources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"- {doc.page_content[:200]}...\")\n",
    "\n",
    "\n",
    "'''\n",
    "#we can also try this\n",
    "\n",
    "# Access the 'result' key directly\n",
    "retrieved_text = result['result'] if 'result' in result else \"No context available.\"\n",
    "\n",
    "# Create the prompt using the retrieved text\n",
    "prompt = f\"Use the following information to answer the question:\\n{retrieved_text}\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "# Generate the response using the prompt\n",
    "response = generate_response(prompt)\n",
    "\n",
    "# Display the question and answer\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", response)\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "if we have the error: Token indices sequence length is longer than the specified maximum sequence length for this model (1255 > 512). Running this sequence through the model will result in indexing errors\n",
    "\n",
    "we can tokenize query with truncation:\n",
    "example:\n",
    "query = \"How do I book an appointment with an advisor?\"\n",
    "\n",
    "# Tokenize with truncation\n",
    "tokenized_output = tokenizer(\n",
    "    query,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=512,        # Truncate input to fit within the model's limit\n",
    "    truncation=True        # Automatically truncate if input is too long\n",
    ")\n",
    "\n",
    "# Decode truncated input\n",
    "truncated_query = tokenizer.decode(tokenized_output[\"input_ids\"][0], skip_special_tokens=True)\n",
    "print(\"\\nTruncated Query:\")\n",
    "print(truncated_query)\n",
    "\n",
    "# Generate response\n",
    "response = text_generator(truncated_query)\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response[0]['generated_text'])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b429b82f-a765-4d2e-a643-012f084b1a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " https://immigration-advisors.youcanbook.me/ Meet a Student Life Advisor: https://student-life-advisors.youcanbook.me/\n",
      "\n",
      "Sources:\n",
      "- email intl-sponsor@pdx.edu. You can meet with any International Student advisor for questions about your F-1 or J-1 status. Visiting Scholars: Visiting scholars, faculty, and researchers on the J-1 vi...\n",
      "- International Students ADVISING SERVICES IKARL MILLER CENTER & State FRONT DESK SERVICES For general information and quick questions, visit our front desk services. A staff or student member can chat ...\n",
      "- divide document review requests based on PSU ID so that requests can be processed consistently for all students. You can meet with any International Student Advisor whose availability fits your schedu...\n"
     ]
    }
   ],
   "source": [
    "query = \"what website can i go to book appointment with an advisor?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# Output the answer\n",
    "print(\"\\nAnswer:\\n\", result[\"result\"])\n",
    "\n",
    "# (Optional) Show which chunks were used\n",
    "print(\"\\nSources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"- {doc.page_content[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7476c148-6601-4cd8-b617-abcc16fac2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " You are an expert assistant. user where is kmc? model : f x (0) in 2/2 International Students ADVISING SERVICES IKARL MILLER CENTER & State FRONT DESK SERVICES For general information and quick questions, visit our front desk services. A staff or student member can chat with you and provide information, resources, and help schedule an appointment with an International Student Advisor. user Where is KMC? model\n"
     ]
    }
   ],
   "source": [
    "#if working with chat state, this helps maintain history\n",
    "\n",
    "query = \"where is KMC located?\"\n",
    "\n",
    "# Add user query to history\n",
    "chat_state.add_to_history_as_user(query)\n",
    "\n",
    "# Retrieve relevant context\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "# Build full prompt with retrieved context\n",
    "full_prompt = f\"{context}\\n\\n{chat_state.get_full_prompt()}\"\n",
    "\n",
    "# Get model response\n",
    "response = llm.invoke(full_prompt)\n",
    "#or using kerasnlpmodel\n",
    "#gemma_lm.generate(prompt, max_length=256)\n",
    "\n",
    "# Store the model's response in the chat history\n",
    "chat_state.add_to_history_as_model(response)\n",
    "\n",
    "# Output the result\n",
    "print(\"\\nAnswer:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b4cf04e-c653-4cf7-9588-f477a82121e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chat_state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhere is KMC located?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Add user query to history\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mchat_state\u001b[49m\u001b[38;5;241m.\u001b[39madd_to_history_as_user(query)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Retrieve relevant context\u001b[39;00m\n\u001b[1;32m     10\u001b[0m retrieved_docs \u001b[38;5;241m=\u001b[39m retriever\u001b[38;5;241m.\u001b[39mget_relevant_documents(query)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chat_state' is not defined"
     ]
    }
   ],
   "source": [
    "#instead of the cell above use below to avoid error in indexing due to long input and context due to 512 max capacity. see last cell\n",
    "\n",
    "# If working with chat state, this helps maintain history\n",
    "query = \"Where is KMC located?\"\n",
    "\n",
    "# Add user query to history\n",
    "chat_state.add_to_history_as_user(query)\n",
    "\n",
    "# Retrieve relevant context\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Inspect retrieved documents\n",
    "print(\"Retrieved Documents:\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"Document {i + 1}: {doc.page_content[:200]}...\")\n",
    "    \n",
    "context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "# Build full prompt with retrieved context\n",
    "full_prompt = f\"{context}\\n\\n{chat_state.get_full_prompt()}\"\n",
    "\n",
    "# Tokenize and truncate the full prompt to fit within the model's token limit\n",
    "max_input_length = 512  # Maximum input length for google/flan-t5-large\n",
    "max_query_length = 128  # Reserve space for the query\n",
    "max_context_length = max_input_length - max_query_length\n",
    "# Truncate context\n",
    "truncated_context = tokenizer.decode(\n",
    "    tokenizer(context, max_length=max_context_length, truncation=True)[\"input_ids\"],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# Build full prompt with truncated context\n",
    "full_prompt = f\"\"\"Answer the following question based on the provided context.\n",
    "\n",
    "Context:\n",
    "{truncated_context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Tokenize and truncate the full prompt\n",
    "tokenized_output = tokenizer(\n",
    "    full_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=max_input_length,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# Decode truncated input\n",
    "truncated_input = tokenizer.decode(tokenized_output[\"input_ids\"][0], skip_special_tokens=True)\n",
    "print(\"\\nTruncated Input:\")\n",
    "print(truncated_input)\n",
    "\n",
    "# Generate response using the truncated input\n",
    "response = text_generator(truncated_input)\n",
    "\n",
    "# Store the model's response in the chat history\n",
    "chat_state.add_to_history_as_model(response[0]['generated_text'])\n",
    "\n",
    "# Output the result\n",
    "print(\"\\nAnswer:\\n\", response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bdb47b-0270-4290-8003-7486cb193c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fddbd4c-763e-4815-aa7a-34798326ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class tries to combine the 2 main functions above: The combined class handles both external retrieval and turn-based chat management.\n",
    "class ChatState:\n",
    "    def __init__(self, retriever, tokenizer, text_generator, max_input_length=512, max_query_length=128, system=\"\"):\n",
    "        \"\"\"\n",
    "        Initialize the ChatState object.\n",
    "        \n",
    "        Args:\n",
    "            retriever: Vector store retriever for fetching relevant documents.\n",
    "            tokenizer: Tokenizer for the language model.\n",
    "            text_generator: Text generation pipeline.\n",
    "            max_input_length (int): Maximum input length for the model.\n",
    "            max_query_length (int): Reserved space for the query.\n",
    "            system (str): System instructions or bot description.\n",
    "        \"\"\"\n",
    "        self.retriever = retriever\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_generator = text_generator\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_query_length = max_query_length\n",
    "        self.system = system\n",
    "        self.history = []\n",
    "\n",
    "    def add_to_history_as_user(self, message):\n",
    "        \"\"\"Add a user message to the chat history.\"\"\"\n",
    "        self.history.append(f\"user\\n{message}\\n\")\n",
    "\n",
    "    def add_to_history_as_model(self, message):\n",
    "        \"\"\"Add a model message to the chat history.\"\"\"\n",
    "        self.history.append(f\"model\\n{message}\\n\")\n",
    "\n",
    "    def get_full_prompt(self):\n",
    "        \"\"\"Combine chat history into a single string.\"\"\"\n",
    "        return \"\".join([*self.history])\n",
    "\n",
    "    def send_message(self, query):\n",
    "        \"\"\"\n",
    "        Process a user query, generate a response, and update the chat history.\n",
    "        \n",
    "        Args:\n",
    "            query (str): The user's question.\n",
    "        \n",
    "        Returns:\n",
    "            str: The generated response.\n",
    "        \"\"\"\n",
    "        # Add user query to history\n",
    "        self.add_to_history_as_user(query)\n",
    "\n",
    "        # Retrieve relevant documents\n",
    "        retrieved_docs = self.retriever.get_relevant_documents(query)\n",
    "        print(\"Retrieved Documents:\")\n",
    "        for i, doc in enumerate(retrieved_docs):\n",
    "            print(f\"Document {i + 1}: {doc.page_content[:200]}...\")\n",
    "        \n",
    "        context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "        # Define token limits\n",
    "        max_context_length = self.max_input_length - self.max_query_length\n",
    "\n",
    "        # Truncate context\n",
    "        truncated_context = self.tokenizer.decode(\n",
    "            self.tokenizer(context, max_length=max_context_length, truncation=True)[\"input_ids\"],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # Build full prompt with truncated context\n",
    "        full_prompt = f\"\"\"Answer the following question based on the provided context.\n",
    "\n",
    "Context:\n",
    "{truncated_context}\n",
    "\n",
    "Chat History:\n",
    "{self.get_full_prompt()}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        # Tokenize and truncate the full prompt\n",
    "        tokenized_output = self.tokenizer(\n",
    "            full_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_input_length,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        # Decode truncated input\n",
    "        truncated_input = self.tokenizer.decode(tokenized_output[\"input_ids\"][0], skip_special_tokens=True)\n",
    "        print(\"\\nTruncated Input:\")\n",
    "        print(truncated_input)\n",
    "\n",
    "        # Generate response using the truncated input\n",
    "        response = self.text_generator(truncated_input)\n",
    "\n",
    "        # Store the model's response in the chat history\n",
    "        self.add_to_history_as_model(response[0]['generated_text'])\n",
    "\n",
    "        # Return the generated response\n",
    "        return response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1c0ff6ef-1f3e-4176-b2a1-4a7336c39183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents:\n",
      "Document 1: age, ability or income level. Multnomah County Child Care Centers Listing Compiled by the Early Learning Division, Oregon Dept of Education Multnomah County Child Care Family Home Listing Compiled by ...\n",
      "Document 2: International Students ADVISING SERVICES IKARL MILLER CENTER & State FRONT DESK SERVICES For general information and quick questions, visit our front desk services. A staff or student member can chat ...\n",
      "Document 3: needs Oregon Early Learning Division Oregon Early Learning Division - Request Complaint and Compliance History - Child Care Providers CONTACT PSU 1825 SW Broadway Portland, OR 97201 Phone: 503-725-300...\n",
      "\n",
      "Truncated Input:\n",
      "Answer the following question based on the provided context. Context: age, ability or income level. Multnomah County Child Care Centers Listing Compiled by the Early Learning Division, Oregon Dept of Education Multnomah County Child Care Family Home Listing Compiled by the Early Learning Division, Oregon Dept of Education 1/2 Find ChildCare Oregon Find Child Care Oregon includes resources, information, tips, and strategies to finding the childcare that's right for you. They even have an online childcare search tool Child Care Assistance Oregon's Department of Human Services manages the Employment-Related Day Care program (ERDC). ERDC helps eligible low-income families pay for child care while they are working. Central Coordination Child Care Resource and Referral Resources to help you meet your child care needs Oregon Early Learning Division Oregon Early Learning Division - Request Complaint and Compliance History - Child Care Providers CONTACT PSU 1825 SW Broadway Portland, OR 97201 Phone: 503-725-3000 Contact Us LEGAL LEARN MORE ADA Accessibility International Students ADVISING SERVICES IKARL MILLER CENTER & State FRONT DESK SERVICES For general information and quick questions, visit our front desk services. A staff or student member can chat with you and provide information, resources, and help schedule an appointment with an International Student Advisor. Virtual Front Desk Hours Monday - Friday 11:00am - 1:00pm Access the Virtual Front Desk In-person Front Desk Hours (Summer) Karl Miller Center, Suite 660 Monday to Friday from 9:00am - 5:00pm University Center Building, Suite 400 Monday to Friday from 9:00am - 3:00pm 1/9 STOPPING OR PARKING EXIT OPEN A VISIT THE VIRTUAL FRONT DESK >> 323 C DOLL WHO IS MY ADVIS Chat History: User: Where is KMC located? Question: Where is KMC located? Answer:\n",
      "\n",
      "Answer:\n",
      " (iv) .. a., b., c., d., e., f., g.\n"
     ]
    }
   ],
   "source": [
    "chat_state = ChatState(retriever, tokenizer, text_generator)\n",
    "response = chat_state.send_message(\"Where is KMC located?\")\n",
    "print(\"\\nAnswer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b051848-9f9f-4fdf-bfa5-d80a64d747e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e86fcd7-f350-46e8-beec-0d51f33018fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb352813-6d3f-47de-935d-853341a6c078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf91011b-13a9-4a35-afd7-442832f77e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abf9451-08c6-480f-bf56-5e17b0d1a9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d139babb-0ca6-4f85-a500-1233338129f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument 'prompts' is expected to be of type List[str], received argument of type <class 'str'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#to maintain state across turns\u001b[39;00m\n\u001b[1;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is the meaning of kmc?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchat_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#print(\"\\nChat History:\\n\", chat_state.get_history())\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[84], line 65\u001b[0m, in \u001b[0;36mChatState.send_message\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_to_history_as_user(message)\n\u001b[1;32m     64\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_full_prompt()\n\u001b[0;32m---> 65\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m result \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mreplace(prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Extract only the new response\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_to_history_as_model(result)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_core/language_models/llms.py:857\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompts, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    853\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    854\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompts\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is expected to be of type List[str], received\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m argument of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    856\u001b[0m     )\n\u001b[0;32m--> 857\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)  \u001b[38;5;66;03m# noqa: TRY004\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;66;03m# Create callback managers\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(metadata, \u001b[38;5;28mlist\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: Argument 'prompts' is expected to be of type List[str], received argument of type <class 'str'>."
     ]
    }
   ],
   "source": [
    "#to maintain state across turns\n",
    "query = \"what is the meaning of kmc?\"\n",
    "response = chat_state.send_message(query)\n",
    "print(\"\\nAnswer:\\n\", response)\n",
    "#print(\"\\nChat History:\\n\", chat_state.get_history())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60720c40-d2a3-4930-862a-f9c82f6087f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using chat state alongside retriever\n",
    "\n",
    "# Initialize the ChatState with your model\n",
    "chat_state = ChatState(model=llm, system=\"You are a helpful assistant.\")\n",
    "\n",
    "# User's input\n",
    "user_message = \"What are the latest advancements in AI?\"\n",
    "\n",
    "# Add to history and create prompt\n",
    "chat_state.add_to_history_as_user(user_message)\n",
    "\n",
    "# Get the full conversation history\n",
    "history = chat_state.get_full_prompt()\n",
    "\n",
    "# Use the retriever to get relevant context\n",
    "retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"k\": 5, \"score_threshold\": 0.5})\n",
    "\n",
    "# Retrieve documents for context\n",
    "context_docs = retriever.retrieve(user_message)  # You can also pass in `history` as context here\n",
    "\n",
    "# Now pass the history and context to the model prompt\n",
    "response = llm.generate(history + \"\\n\" + context_docs)\n",
    "\n",
    "# Add response to history\n",
    "chat_state.add_to_history_as_model(response)\n",
    "\n",
    "# Output response\n",
    "print(\"Model response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536b9e72-3317-42b9-a980-c496cb530fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add interface for ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86fff0-d9be-4a66-adcd-9c77c0cd943e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf020e8-d951-4e66-9208-df14b1a53593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7558987a-bb0a-45fb-9167-214dd6a621d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5fb3eb-9d50-4c3f-ad73-6328033e9a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccca756-9784-4d59-98d0-cc1dbe1b5633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e65a19f-1700-4055-ab94-d2bbf492cfd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42445c53-7608-4568-b707-2e851062d62c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6340ea4-fbc6-4662-8f6a-b2b0297a7b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
