{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3322d010-5ce6-470c-ade8-b86f7f49b31f",
   "metadata": {},
   "source": [
    "# Enviroment\n",
    "\n",
    "This notebook was tested in the following environment:\n",
    "\n",
    "- Python 3.10.15\n",
    "- Managed Notebook with a g2-standard-24 runtime on TensorFlow 2.12 (Local) Kernel:\n",
    "    - 24 vCPUs\n",
    "    - 96 GB RAM\n",
    "    - 2 NVIDIA L4 GPUs\n",
    "   \n",
    "The components of this environment use the billable components of google cloud - Vertex AI and Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ef1603-3c76-4a0d-8f0b-e36526829a52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.15\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97c7d4-a59a-4d07-9abd-9cf66bc2c186",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "* Load Gemma2 using KerasNLP\n",
    "* Finetune Gemma using KerasNLP\n",
    "* Publish finetuned Keras model to kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342a7f6-b90f-4578-a737-251fa96cc278",
   "metadata": {},
   "source": [
    "# About the Model\n",
    "Gemma2 - the version 2 of Gemma,  is a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915cd993-f326-40e9-bcf6-67e08b098bed",
   "metadata": {},
   "source": [
    "# Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b16d4d17-e238-4357-8c0d-662e27c8b108",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: jax 0.4.30\n",
      "Uninstalling jax-0.4.30:\n",
      "  Successfully uninstalled jax-0.4.30\n",
      "Found existing installation: jaxlib 0.4.30\n",
      "Uninstalling jaxlib-0.4.30:\n",
      "  Successfully uninstalled jaxlib-0.4.30\n",
      "\u001b[33mWARNING: Skipping jaxtyping as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: keras 2.12.0\n",
      "Uninstalling keras-2.12.0:\n",
      "  Successfully uninstalled keras-2.12.0\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (24.2)\n",
      "Collecting pip\n",
      "  Using cached pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Using cached pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.2\n",
      "    Uninstalling pip-24.2:\n",
      "      Successfully uninstalled pip-24.2\n",
      "Successfully installed pip-24.3.1\n",
      "Looking in links: https://storage.googleapis.com/jax-release/jax_cuda_releases.html\n",
      "Collecting jax[cuda12_pip]\n",
      "  Using cached jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax[cuda12_pip])\n",
      "  Using cached jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
      "Requirement already satisfied: ml-dtypes>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from jax[cuda12_pip]) (0.5.0)\n",
      "Collecting numpy>=1.24 (from jax[cuda12_pip])\n",
      "  Using cached numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: opt-einsum in /opt/conda/lib/python3.10/site-packages (from jax[cuda12_pip]) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.10 in /opt/conda/lib/python3.10/site-packages (from jax[cuda12_pip]) (1.11.4)\n",
      "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax[cuda12_pip])\n",
      "  Using cached jaxlib-0.4.34-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
      "Collecting jax-cuda12-plugin<=0.4.35,>=0.4.34 (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip])\n",
      "  Using cached jax_cuda12_plugin-0.4.35-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting jax-cuda12-pjrt==0.4.35 (from jax-cuda12-plugin<=0.4.35,>=0.4.34->jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip])\n",
      "  Using cached jax_cuda12_pjrt-0.4.35-py3-none-manylinux2014_x86_64.whl.metadata (349 bytes)\n",
      "Collecting nvidia-cublas-cu12>=12.1.3.1 (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip])\n",
      "  Using cached nvidia_cublas_cu12-12.6.3.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12>=12.1.105 (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip])\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-nvcc-cu12>=12.1.105 (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip])\n",
      "  Using cached nvidia_cuda_nvcc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12>=12.1.105 (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip])\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cudnn-cu12<10.0,>=9.1 (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip])\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu12>=11.0.2.54 (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip])\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12>=11.4.5.107 (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip])\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12>=12.1.0.106 (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip])\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12>=2.18.1 (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip])\n",
      "  Using cached nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12>=12.1.105 (from jax-cuda12-plugin[with_cuda]<=0.4.35,>=0.4.34; extra == \"cuda12-pip\"->jax[cuda12_pip])\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting numpy>=1.24 (from jax[cuda12_pip])\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Using cached jaxlib-0.4.34-cp310-cp310-manylinux2014_x86_64.whl (86.1 MB)\n",
      "Using cached jax_cuda12_plugin-0.4.35-cp310-cp310-manylinux2014_x86_64.whl (15.5 MB)\n",
      "Using cached jax_cuda12_pjrt-0.4.35-py3-none-manylinux2014_x86_64.whl (100.8 MB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached jax-0.4.35-py3-none-any.whl (2.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.6.3.3-py3-none-manylinux2014_x86_64.whl (393.1 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.whl (8.9 MB)\n",
      "Using cached nvidia_cuda_nvcc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (21.2 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (897 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.whl (200.2 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.whl (158.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.whl (216.6 MB)\n",
      "Using cached nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl (199.0 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "Installing collected packages: jax-cuda12-pjrt, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvcc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, jax-cuda12-plugin, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, jaxlib, jax\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.12.0 requires keras<2.13,>=2.12.0, which is not installed.\n",
      "apache-beam 2.46.0 requires grpcio!=1.48.0,<2,>=1.33.1, but you have grpcio 1.48.0 which is incompatible.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n",
      "tensorboard 2.12.3 requires grpcio>=1.48.2, but you have grpcio 1.48.0 which is incompatible.\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.26.4 which is incompatible.\n",
      "ydata-profiling 4.6.0 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed jax-0.4.35 jax-cuda12-pjrt-0.4.35 jax-cuda12-plugin-0.4.35 jaxlib-0.4.34 numpy-1.26.4 nvidia-cublas-cu12-12.6.3.3 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvcc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-nccl-cu12-2.23.4 nvidia-nvjitlink-cu12-12.6.77\n",
      "Collecting keras\n",
      "  Using cached keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras) (1.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras) (13.8.1)\n",
      "Collecting namex (from keras)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras) (3.11.0)\n",
      "Collecting optree (from keras)\n",
      "  Using cached optree-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "Requirement already satisfied: ml-dtypes in /opt/conda/lib/python3.10/site-packages (from keras) (0.5.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from optree->keras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Using cached keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "Installing collected packages: namex, optree, keras\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.12.0 requires keras<2.13,>=2.12.0, but you have keras 3.6.0 which is incompatible.\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-3.6.0 namex-0.0.8 optree-0.13.0\n",
      "Requirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras) (1.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras) (13.8.1)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras) (0.13.0)\n",
      "Requirement already satisfied: ml-dtypes in /opt/conda/lib/python3.10/site-packages (from keras) (0.5.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from optree->keras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Collecting keras-nlp\n",
      "  Using cached keras_nlp-0.17.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting keras-hub==0.17.0 (from keras-nlp)\n",
      "  Using cached keras_hub-0.17.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras-hub==0.17.0->keras-nlp) (1.4.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras-hub==0.17.0->keras-nlp) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras-hub==0.17.0->keras-nlp) (24.1)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from keras-hub==0.17.0->keras-nlp) (2024.9.11)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras-hub==0.17.0->keras-nlp) (13.8.1)\n",
      "Collecting kagglehub (from keras-hub==0.17.0->keras-nlp)\n",
      "  Using cached kagglehub-0.3.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tensorflow-text (from keras-hub==0.17.0->keras-nlp)\n",
      "  Using cached tensorflow_text-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kagglehub->keras-hub==0.17.0->keras-nlp) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kagglehub->keras-hub==0.17.0->keras-nlp) (4.66.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-hub==0.17.0->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-hub==0.17.0->keras-nlp) (2.18.0)\n",
      "Collecting tensorflow<2.19,>=2.18.0 (from tensorflow-text->keras-hub==0.17.0->keras-nlp)\n",
      "  Using cached tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-hub==0.17.0->keras-nlp) (0.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (74.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (1.48.0)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (3.6.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (3.11.0)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp)\n",
      "  Using cached ml_dtypes-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (0.32.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.17.0->keras-nlp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.17.0->keras-nlp) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.17.0->keras-nlp) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub->keras-hub==0.17.0->keras-nlp) (2024.8.30)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (0.44.0)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (0.13.0)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp)\n",
      "  Using cached grpcio-1.67.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow-text->keras-hub==0.17.0->keras-nlp) (2.1.2)\n",
      "Using cached keras_nlp-0.17.0-py3-none-any.whl (2.0 kB)\n",
      "Using cached keras_hub-0.17.0-py3-none-any.whl (644 kB)\n",
      "Using cached kagglehub-0.3.3-py3-none-any.whl (42 kB)\n",
      "Using cached tensorflow_text-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "Using cached tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
      "Using cached ml_dtypes-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached grpcio-1.67.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "Installing collected packages: ml-dtypes, grpcio, tensorboard, kagglehub, tensorflow, tensorflow-text, keras-hub, keras-nlp\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml_dtypes 0.5.0\n",
      "    Uninstalling ml_dtypes-0.5.0:\n",
      "      Successfully uninstalled ml_dtypes-0.5.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.48.0\n",
      "    Uninstalling grpcio-1.48.0:\n",
      "      Successfully uninstalled grpcio-1.48.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.12.3\n",
      "    Uninstalling tensorboard-2.12.3:\n",
      "      Successfully uninstalled tensorboard-2.12.3\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.12.0\n",
      "    Uninstalling tensorflow-2.12.0:\n",
      "      Successfully uninstalled tensorflow-2.12.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed grpcio-1.66.1 kagglehub-0.3.3 keras-hub-0.17.0 keras-nlp-0.17.0 ml-dtypes-0.4.1 tensorboard-2.18.0 tensorflow-2.18.0 tensorflow-text-2.18.0\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall jax jaxlib jaxtyping -y\n",
    "!pip uninstall -y keras\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-release/jax_cuda_releases.html\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend\n",
    "!pip install keras\n",
    "!pip install --upgrade keras\n",
    "!pip install keras-nlp\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca3afc4c-06c4-4fcf-8db9-a6a1113b73a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Using cached accelerate-1.1.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\n",
      "Collecting torch>=1.10.0 (from accelerate)\n",
      "  Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting huggingface-hub>=0.21.0 (from accelerate)\n",
      "  Using cached huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors>=0.4.3 (from accelerate)\n",
      "  Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Using cached tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.10.0->accelerate)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch>=1.10.0->accelerate)\n",
      "  Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.10.0->accelerate)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.10.0->accelerate)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.0.1)\n",
      "Using cached accelerate-1.1.0-py3-none-any.whl (333 kB)\n",
      "Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
      "Using cached huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Using cached tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: sentencepiece, mpmath, triton, sympy, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch, accelerate\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.6.77\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
      "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "Successfully installed accelerate-1.1.0 huggingface-hub-0.26.2 mpmath-1.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 safetensors-0.4.5 sentencepiece-0.2.0 sympy-1.13.1 tokenizers-0.20.3 torch-2.5.1 transformers-4.46.2 triton-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.10/site-packages (1.67.1)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Using cached google_cloud_aiplatform-1.71.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.34.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.35.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (24.1)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.14.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.25.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.12.5)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.0.6)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.10.18)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.65.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.66.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.9.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (4.12.2)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.26.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2024.8.30)\n",
      "Using cached google_cloud_aiplatform-1.71.1-py2.py3-none-any.whl (6.2 MB)\n",
      "Installing collected packages: google-cloud-aiplatform\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.67.1\n",
      "    Uninstalling google-cloud-aiplatform-1.67.1:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.67.1\n",
      "Successfully installed google-cloud-aiplatform-1.71.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.5)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.6-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.0.1)\n",
      "Using cached tqdm-4.66.6-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.5\n",
      "    Uninstalling tqdm-4.66.5:\n",
      "      Successfully uninstalled tqdm-4.66.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ydata-profiling 4.6.0 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tqdm-4.66.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pycuda\n",
      "  Using cached pycuda-2024.1.2-cp310-cp310-linux_x86_64.whl\n",
      "Collecting pytools>=2011.2 (from pycuda)\n",
      "  Using cached pytools-2024.1.14-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from pycuda) (4.3.6)\n",
      "Collecting mako (from pycuda)\n",
      "  Using cached Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from pytools>=2011.2->pycuda) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from mako->pycuda) (2.0.1)\n",
      "Using cached pytools-2024.1.14-py3-none-any.whl (89 kB)\n",
      "Using cached Mako-1.3.6-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: pytools, mako, pycuda\n",
      "Successfully installed mako-1.3.6 pycuda-2024.1.2 pytools-2024.1.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade accelerate sentencepiece transformers\n",
    "%pip install --upgrade google-cloud-aiplatform\n",
    "%pip install --upgrade tqdm torch\n",
    "%pip install pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "982d3949-e6b1-49d8-a166-37be2353f150",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install --upgrade --quiet kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f23a0ac-2da6-4249-8870-2a525d01c47b",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc482ff-931a-4882-bcf6-a6e02cf9d342",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39859a3c8d6f49119f1205a1472a0c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "kagglehub.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f01e27e3-2908-4038-979e-546339f88030",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 08:32:10.641919: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730881930.665329       1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730881930.672134       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "import locale\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import keras\n",
    "import keras_nlp\n",
    "import torch\n",
    "import transformers\n",
    "from google.cloud import aiplatform\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit  # This automatically initializes CUDA\n",
    "import textwrap\n",
    "from IPython.display import Markdown, display\n",
    "import keras_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071ecfc3-f045-4061-b323-a7ec930877bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for gpu in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66120d9c-a4eb-4216-97d1-24187fd54144",
   "metadata": {},
   "source": [
    "## Load fine-tuning data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b68254-7b50-4904-9ee2-c495fb5b34a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE the path to your dataset is likely different\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "ft_df = pd.read_csv(\"bias-dataset.csv\")\n",
    "#ft_df.describe()\n",
    "ft_df.head(3)\n",
    "#ft_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7523436b-f8df-49ae-a177-01ff9c6b7fc3",
   "metadata": {},
   "source": [
    "## Defining Formatting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79b6678c-bbaa-4f02-93ac-c6183cade907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_chat(prompt, response):\n",
    "  '''Displays an LLM prompt and response in a pretty way.'''\n",
    "  prompt = prompt.replace('\\n\\n','<br><br>')\n",
    "  prompt = prompt.replace('\\n','<br>')\n",
    "  formatted_prompt = \"<font size='+1' color='pink'><blockquote>\" + prompt + \"</blockquote></font>\"\n",
    "  response = response.replace('', '  *')\n",
    "  response = textwrap.indent(response, '', predicate=lambda _: True)\n",
    "  response = response.replace('\\n\\n','<br><br>')\n",
    "  response = response.replace('\\n','<br>')\n",
    "  response = response.replace(\"```\",\"\")\n",
    "  formatted_text = \"<font size='+1' color='lightblue'><blockquote>\" + response + \"</blockquote></font>\"\n",
    "  return Markdown(formatted_prompt+formatted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e31ee34-901a-4152-8ed2-d08c06f1c053",
   "metadata": {},
   "source": [
    "## Load Gemma2 instruct 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8936a90-9175-4d75-97db-a4d6e77d7cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1730874354.640252       1 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1632 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "I0000 00:00:1730874354.642584       1 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 20748 MB memory:  -> device: 1, name: NVIDIA L4, pci bus id: 0000:00:04.0, compute capability: 8.9\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                                                  </span><span style=\"font-weight: bold\">                                   Config </span>\n",
       "\n",
       " gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                                                    Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                                                    Vocab size: \u001b[38;5;34m256,000\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                  </span><span style=\"font-weight: bold\"> Output Shape              </span><span style=\"font-weight: bold\">         Param # </span><span style=\"font-weight: bold\"> Connected to               </span>\n",
       "\n",
       " padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n",
       "\n",
       " token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n",
       "\n",
       " gemma_backbone                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span>  padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)                                                            token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "\n",
       " token_embedding                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span>  gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)                                                                                 \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n",
       "\n",
       " token_ids (\u001b[38;5;33mInputLayer\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n",
       "\n",
       " gemma_backbone                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)           \u001b[38;5;34m2,614,341,888\u001b[0m  padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        \n",
       " (\u001b[38;5;33mGemmaBackbone\u001b[0m)                                                            token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "\n",
       " token_embedding                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)           \u001b[38;5;34m589,824,000\u001b[0m  gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       " (\u001b[38;5;33mReversibleEmbedding\u001b[0m)                                                                                 \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_2b_en\")\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e743cbd8-97e2-42ac-97bd-ccc67363c2c6",
   "metadata": {},
   "source": [
    "## Testing A Prompt\n",
    "* we don't expected it to do well esp. given the specificity of the disclaimers in the fine-tuning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d7acd72-624d-4a80-a72f-ad978c35cfb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 06:28:37.756362: E tensorflow/core/util/util.cc:131] oneDNN supports DT_INT64 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='pink'><blockquote>You are an AI assistant that translates an input sentence into an output sentence.  Add a disclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>Output:<br></blockquote></font><font size='+1' color='lightblue'><blockquote>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The input sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The output sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The input sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The output sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The input sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The output sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The input sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The output sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The input sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The output sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The input sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The output sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The input sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The output sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The input sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The output sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The input sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The output sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The input sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The output sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The input sentence is:<br>Write 32 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.<br><br>The output sentence</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"{pre}\\n\\nInput:\\n{src}\\n\\nOutput:\\n{target}\"\n",
    "prompt = prompt_template.format(\n",
    "    pre='''You are an AI assistant that translates an input sentence '''\\\n",
    "        '''into an output sentence.  Add a disclaimer to the input that indicates '''\\\n",
    "        '''how to specifically ensure balanced representation of genders and ethnicities.''',\n",
    "    src='''Write 32 different stories about interacting with 24 different professionals that work'''\n",
    "        ''' in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.''',\n",
    "    target=\"\"\n",
    ")\n",
    "completion = gemma_lm.generate(prompt, max_length=1024)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c4f1e1-b5d1-4401-a77a-6ad4026981db",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "* The un-fined-model seems to try hard but fails on specifics.\n",
    "* Let's create the finetuning dataset to start out fine-tuning experiments.\n",
    "\n",
    "## Create the fine-tuning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "741a008c-98c2-4159-8c22-84cedadab7d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total num ft items= 200\n",
      "max token length= 281\n",
      "ft data example= The following is an excerpt from a conversation between an AI assistant and user. It demonstrates how the AI assistant translates an input sentence into an output sentence.  The AI assistant adds a disclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.\n",
      "\n",
      "Input:\n",
      "write 24 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Please create each story in such a way they have an ethnicity and gender.\n",
      "\n",
      "Output:\n",
      "write 24 different stories about interacting with 24 different professionals that work in Architectural and Engineering. Ensure that the stories collectively represent an equal gender distribution (50% Female, 50% Male) and an ethnically diverse background as follows: European (25%), African (25%), Hispanic/Latino (25%), and API (25%). Within each ethnic group, maintain a balanced representation of males and females (12.5% each).\n",
      "\n",
      "ft train item=\n",
      " query=\n",
      " create 24 different stories about interacting with 24 different professionals that work in Architectural and Engineering. \n",
      "response=\n",
      " Create 24 different stories about interacting with 24 different professionals that work in Architecture and Engineering. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders even in each ethnicity. Avoid overrepresenting any particular ethnicity or gender.\n",
      "while creating each story put into consideration that:\n",
      "1) there has been a significant increase in the number of API individuals pursuing higher education and careers in Architecture and Engineering. As a result, you are likely to generate more Asian characters than other ethnicities.\n",
      "Therefore, do not overrepresent Asian ethnicities.\n",
      "\n",
      "ft test item=\n",
      " query=\n",
      " create 16 different stories about interacting with 16 different professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity \n",
      "response=\n",
      " create 16 different stories about interacting with 16 different professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity. While creating each story put into consideration that you are likely to overrepresent Asians in this data because of their significant contributions to the global transportation and logistics industries. Therefore, avoid overrepresenting Asians. Make sure all ethnicities are equally represented and genders are equally represented.\n"
     ]
    }
   ],
   "source": [
    "# Define a special template for each fine-tuning training item\n",
    "ft_template = \"{pre}\\n\\nInput:\\n{src}\\n\\nOutput:\\n{target}\"\n",
    "\n",
    "# Get the Gemma2 tokenizer - used later to get the token len of each string\n",
    "tokenizer = keras_hub.models.GemmaTokenizer.from_preset(\"gemma2_2b_en\")\n",
    "\n",
    "ft_all_data = []\n",
    "ft_train = None\n",
    "ft_test = None\n",
    "max_tokens = 0\n",
    "for idx, row in ft_df.iterrows():\n",
    "\n",
    "  if idx==2: ft_train = row # remember the first row, we will need this later for model eval\n",
    "\n",
    "  if idx == 200: # We are only using the first half\n",
    "    break\n",
    "\n",
    "  ft_item = ft_template.format(\n",
    "    pre='''The following is an excerpt from a conversation between an AI assistant and user. '''\\\n",
    "        '''It demonstrates how the AI assistant translates an input sentence '''\\\n",
    "        '''into an output sentence.  The AI assistant adds a disclaimer to the input that indicates '''\\\n",
    "        '''how to specifically ensure balanced representation of genders and ethnicities.''',\n",
    "    src=row['Query'].strip(),\n",
    "    target=row['Response'].strip()\n",
    "  )\n",
    "  # get number of tokens of this itme\n",
    "  tokenized_input = tokenizer.tokenize(ft_item)\n",
    "  token_len = len(tokenized_input)\n",
    "  if token_len>1024:\n",
    "    # NOTE: the fine-tuning code later on trains of token lengths of 1024\n",
    "    # NOTE: to limit memory usage.  So, here we filter out strings so that\n",
    "    # NOTE: we don't get truncation of the fine-tuning data.\n",
    "    print(\"skipping - too long\")\n",
    "    continue\n",
    "  if token_len>max_tokens: max_tokens = token_len\n",
    "\n",
    "  # for debugging - print(ft_item)\n",
    "  # for debugging - break\n",
    "  ft_all_data.append(ft_item)\n",
    "\n",
    "  ft_test = row # remember the last row, we will exclude it later from training as use as test item\n",
    "\n",
    "\n",
    "# use all but last row for the fine tuning dataset\n",
    "ft_data = ft_all_data[:-1]\n",
    "print(\"total num ft items=\", len(ft_all_data))\n",
    "print(\"max token length=\", max_tokens)\n",
    "print(\"ft data example=\", ft_all_data[0])\n",
    "print()\n",
    "\n",
    "# we captures specific items for model eval\n",
    "print(\"ft train item=\\n\",\n",
    "      \"query=\\n\",ft_train['Query'].strip(),\n",
    "      \"\\nresponse=\\n\",\n",
    "      ft_train['Response'].strip())\n",
    "print(\"\\nft test item=\\n\",\n",
    "      \"query=\\n\",ft_test['Query'].strip(),\n",
    "      \"\\nresponse=\\n\",\n",
    "      ft_test['Response'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d353bb1-3f98-4534-bb22-d318d527736d",
   "metadata": {},
   "source": [
    "# Eval the in-sample \"train\" prompt on the un-fined-tuned base model (epochs=0)\n",
    "* We don't expect it do well but its a useful baseline as we experiment with fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28a7836d-8230-4e2a-b543-b965beeb296a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='pink'><blockquote>You are an AI assistant that translates an input sentence into an output sentence.  Add a disclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 24 different stories about interacting with 24 different professionals that work in Architectural and Engineering.<br><br>Output:<br></blockquote></font><font size='+1' color='lightblue'><blockquote>create 24 different stories about interacting with 24 different professionals that work in Architectural and Engineering.<br><br>The stories should be written in the first person and should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand.<br><br>The stories should be written in a way that is easy to understand</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(MODEL_NAME)\n",
    "#gemma_lm.summary()\n",
    "\n",
    "prompt_template = \"{pre}\\n\\nInput:\\n{src}\\n\\nOutput:\\n{target}\"\n",
    "prompt = prompt_template.format(\n",
    "    pre='''You are an AI assistant that translates an input sentence '''\\\n",
    "        '''into an output sentence.  Add a disclaimer to the input that indicates '''\\\n",
    "        '''how to specifically ensure balanced representation of genders and ethnicities.''',\n",
    "    src=ft_train['Query'].strip(),\n",
    "    target=\"\"\n",
    ")\n",
    "completion = gemma_lm.generate(prompt, max_length=1024)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f645b0-9ac2-47e0-9112-9dceb4716161",
   "metadata": {},
   "source": [
    "## Eval the out-of-sample \"test\" prompt on the un-fined-tuned model (epochs=0)\n",
    "* we don't expect it to do well but its a useful baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07d8d749-2c02-446a-8a3a-fbb94e04b017",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='pink'><blockquote>You are an AI assistant that translates an input sentence into an output sentence.  Add a disclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 16 different stories about interacting with 16 different professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br></blockquote></font><font size='+1' color='lightblue'><blockquote>16 stories that reflect an equal and balanced proportion of ethnicities and genders within each ethnicity.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The stories should be written in a way that is easy to understand and follow.<br><br>The</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"{pre}\\n\\nInput:\\n{src}\\n\\nOutput:\\n{target}\"\n",
    "prompt = prompt_template.format(\n",
    "    pre='''You are an AI assistant that translates an input sentence '''\\\n",
    "        '''into an output sentence.  Add a disclaimer to the input that indicates '''\\\n",
    "        '''how to specifically ensure balanced representation of genders and ethnicities.''',\n",
    "    src=ft_test['Query'].strip(),\n",
    "    target=\"\"\n",
    ")\n",
    "completion = gemma_lm.generate(prompt, max_length=1024)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de51d583-16fc-454a-b81c-a7c6e66401ec",
   "metadata": {},
   "source": [
    "## FInetuning using LoRA\n",
    "\n",
    "Low Rank Adaptation (LoRA) is a finetuning technique which greatly reduces the number of trainable parameters for downstream tasks by freezing the full weights of the model and inserting a smaller number of new trainable weights into the model. This technique makes training much faster and more memory-efficient.\n",
    "* Let's start fine-tuning experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d05929af-7231-4a05-b67c-57d2109e6388",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                                                  </span><span style=\"font-weight: bold\">                                   Config </span>\n",
       "\n",
       " gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                                                    Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                                                    Vocab size: \u001b[38;5;34m256,000\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                  </span><span style=\"font-weight: bold\"> Output Shape              </span><span style=\"font-weight: bold\">         Param # </span><span style=\"font-weight: bold\"> Connected to               </span>\n",
       "\n",
       " padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n",
       "\n",
       " token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n",
       "\n",
       " gemma_backbone                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span>  padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)                                                            token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "\n",
       " token_embedding                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span>  gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)                                                                                 \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n",
       "\n",
       " token_ids (\u001b[38;5;33mInputLayer\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n",
       "\n",
       " gemma_backbone                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)           \u001b[38;5;34m2,617,270,528\u001b[0m  padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        \n",
       " (\u001b[38;5;33mGemmaBackbone\u001b[0m)                                                            token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "\n",
       " token_embedding                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)           \u001b[38;5;34m589,824,000\u001b[0m  gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       " (\u001b[38;5;33mReversibleEmbedding\u001b[0m)                                                                                 \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,617,270,528</span> (9.75 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,617,270,528\u001b[0m (9.75 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,928,640</span> (11.17 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,928,640\u001b[0m (11.17 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to 4.\n",
    "gemma_lm.backbone.enable_lora(rank=4)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45d86147-8f9f-4257-88f6-9d285fccb23d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(3,)\n",
      "199\n"
     ]
    }
   ],
   "source": [
    "print(ft_test.shape)\n",
    "print(ft_train.shape)\n",
    "print(len(ft_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243346b8-5bc9-43e5-8e1e-13420b584877",
   "metadata": {},
   "source": [
    "\n",
    "## Let's start from 4 epochs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9f7ec2b-3816-4c32-868a-1fbeb7f65690",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1730545641.807424       1 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1632 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "I0000 00:00:1730545641.809084       1 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 20748 MB memory:  -> device: 1, name: NVIDIA L4, pci bus id: 0000:00:04.0, compute capability: 8.9\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                                                  </span><span style=\"font-weight: bold\">                                   Config </span>\n",
       "\n",
       " gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                                                    Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                                                    Vocab size: \u001b[38;5;34m256,000\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                  </span><span style=\"font-weight: bold\"> Output Shape              </span><span style=\"font-weight: bold\">         Param # </span><span style=\"font-weight: bold\"> Connected to               </span>\n",
       "\n",
       " padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n",
       "\n",
       " token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n",
       "\n",
       " gemma_backbone                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span>  padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)                                                            token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "\n",
       " token_embedding                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span>  gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)                                                                                 \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n",
       "\n",
       " token_ids (\u001b[38;5;33mInputLayer\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n",
       "\n",
       " gemma_backbone                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)           \u001b[38;5;34m2,614,341,888\u001b[0m  padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        \n",
       " (\u001b[38;5;33mGemmaBackbone\u001b[0m)                                                            token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "\n",
       " token_embedding                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)           \u001b[38;5;34m589,824,000\u001b[0m  gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       " (\u001b[38;5;33mReversibleEmbedding\u001b[0m)                                                                                 \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_2b_en\")\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac62509-af6e-459c-882f-5d6dda898136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma_lm.backbone.enable_lora(rank=4)\n",
    "# Limit the input sequence length to X (to control memory usage).\n",
    "gemma_lm.preprocessor.sequence_length = 1024 #max_tokens+1\n",
    "# Use AdamW (a common optimizer for transformer models).\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.0, # TODO: Might be worth experimenting with non-zero values\n",
    ")\n",
    "# TODO: Might be worth experiment - Exclude layernorm and bias terms from decay.\n",
    "# TODO: Might be worth experimenting - optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    #sampler =\"greedy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb228aed-c03a-479c-a060-af65dd3c8976",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 11:18:28.357640: E tensorflow/core/util/util.cc:131] oneDNN supports DT_INT64 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 11:19:01.027050: W external/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3020] Can't reduce memory use below 11.57GiB (12427848608 bytes) by rematerialization; only reduced to 15.94GiB (17114700152 bytes), down from 16.71GiB (17947114440 bytes) originally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/199\u001b[0m \u001b[37m\u001b[0m \u001b[1m2:12:17\u001b[0m 40s/step - loss: 0.4348 - sparse_categorical_accuracy: 0.5385"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 11:19:20.159701: W external/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3020] Can't reduce memory use below 11.57GiB (12427848608 bytes) by rematerialization; only reduced to 15.94GiB (17114700152 bytes), down from 16.71GiB (17947114440 bytes) originally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 732ms/step - loss: 0.3069 - sparse_categorical_accuracy: 0.6708\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 11:21:45.847869: W external/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3020] Can't reduce memory use below 11.57GiB (12427848608 bytes) by rematerialization; only reduced to 15.94GiB (17114700152 bytes), down from 16.71GiB (17947114440 bytes) originally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 649ms/step - loss: 0.1195 - sparse_categorical_accuracy: 0.8663\n",
      "Epoch 3/4\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 642ms/step - loss: 0.0984 - sparse_categorical_accuracy: 0.8867\n",
      "Epoch 4/4\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 642ms/step - loss: 0.0854 - sparse_categorical_accuracy: 0.8976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f9ab84c55a0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gemma_lm.fit(ft_data, epochs=4, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5171b460-c87b-4722-84da-03404e1356bd",
   "metadata": {},
   "source": [
    "## Eval the in-sample \"train\" prompt on the model (epoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98004586-6bb7-4474-9b35-05dbb00b75a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='pink'><blockquote>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 24 different stories about interacting with 24 different professionals that work in Architectural and Engineering.<br><br>Output:<br></blockquote></font><font size='+1' color='lightblue'><blockquote>Create 24 different stories about interacting with 24 different professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>Create 24 different stories</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"{pre}\\n\\nInput:\\n{src}\\n\\nOutput:\\n{target}\"\n",
    "prompt = prompt_template.format(\n",
    "    pre='''You are an AI assistant that translates an input sentence '''\\\n",
    "        '''into an output sentence.  Add a diclaimer to the input that indicates '''\\\n",
    "        '''how to specifically ensure balanced representation of genders and ethnicities.''',\n",
    "    src=ft_train['Query'].strip(),\n",
    "    target=\"\"\n",
    ")\n",
    "completion = gemma_lm.generate(prompt, max_length=1024)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a9e8d-2bce-442d-b370-28fdb9519b12",
   "metadata": {},
   "source": [
    "## Eval the out-of-sample \"test\" prompt on the model (epoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a3d02c6-426d-4beb-9b6e-8226c07a8958",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='pink'><blockquote>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 16 different stories about interacting with 16 different professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br></blockquote></font><font size='+1' color='lightblue'><blockquote>Create 16 different stories about interacting with 16 different professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>You are an AI assistant that translates an input sentence into an output sentence. Add a disclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>Create 16 different stories about interacting with 16 different professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br>Create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>You are an AI assistant that translates an input sentence into an output sentence. Add a disclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>Create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br>Create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>You are an AI assistant that translates an input sentence into an output sentence. Add a disclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>Create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br>Create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>You are an AI assistant that translates an input sentence into an output sentence. Add a disclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>Create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br>Create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>You are an AI assistant that translates an input sentence into an output sentence. Add a disclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>Create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br>Create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>You are an AI assistant that translates an input sentence into an output sentence. Add a disclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>Create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br>Create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>You are an AI assistant that translates an input sentence into an output sentence. Add a disclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>Create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br>Create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>You are an AI assistant that translates an input sentence into an output sentence. Add a disclaimer to the input that indicates how to specifically ensure balanced</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"{pre}\\n\\nInput:\\n{src}\\n\\nOutput:\\n{target}\"\n",
    "prompt = prompt_template.format(\n",
    "    pre='''You are an AI assistant that translates an input sentence '''\\\n",
    "        '''into an output sentence.  Add a diclaimer to the input that indicates '''\\\n",
    "        '''how to specifically ensure balanced representation of genders and ethnicities.''',\n",
    "    src=ft_test['Query'].strip(),\n",
    "    target=\"\"\n",
    ")\n",
    "completion = gemma_lm.generate(prompt, max_length=1024)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c6d5c6e-33f7-465e-8fbe-13c7b3c51a71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "\n",
    "# Delete the existing model instance if it exists\n",
    "try:\n",
    "    del gemma_lm\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Clear the Keras backend session\n",
    "K.clear_session()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd0e764-abb0-4185-be30-a7c89551f8f3",
   "metadata": {},
   "source": [
    "## Inference Evaluation\n",
    "* This just repeats the input, therefore, this is not the desired result\n",
    "## let's try 32 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daccccf8-df4d-4344-9d6a-0fcba7133a76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                                                  </span><span style=\"font-weight: bold\">                                   Config </span>\n",
       "\n",
       " gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                                                    Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                                                    Vocab size: \u001b[38;5;34m256,000\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                  </span><span style=\"font-weight: bold\"> Output Shape              </span><span style=\"font-weight: bold\">         Param # </span><span style=\"font-weight: bold\"> Connected to               </span>\n",
       "\n",
       " padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n",
       "\n",
       " token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n",
       "\n",
       " gemma_backbone                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span>  padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)                                                            token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "\n",
       " token_embedding                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span>  gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)                                                                                 \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n",
       "\n",
       " token_ids (\u001b[38;5;33mInputLayer\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n",
       "\n",
       " gemma_backbone                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)           \u001b[38;5;34m2,614,341,888\u001b[0m  padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        \n",
       " (\u001b[38;5;33mGemmaBackbone\u001b[0m)                                                            token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "\n",
       " token_embedding                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)           \u001b[38;5;34m589,824,000\u001b[0m  gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       " (\u001b[38;5;33mReversibleEmbedding\u001b[0m)                                                                                 \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_2b_en\")\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "608bd870-5c23-4412-99c6-6e19a31b80e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov  2 11:55:15 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA L4                      On  |   00000000:00:03.0 Off |                    0 |\n",
      "| N/A   76C    P0             34W /   72W |   22308MiB /  23034MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA L4                      On  |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   76C    P0             34W /   72W |     195MiB /  23034MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67312d9f-7002-436c-b65f-41a17e2b9fa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gemma_lm.backbone.enable_lora(rank=4)\n",
    "# Limit the input sequence length to X (to control memory usage).\n",
    "gemma_lm.preprocessor.sequence_length = 1024 #max_tokens+1\n",
    "# Use AdamW (a common optimizer for transformer models).\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.0, # TODO: Might be worth experimenting with non-zero values\n",
    ")\n",
    "# TODO: Might be worth experiment - Exclude layernorm and bias terms from decay.\n",
    "# TODO: Might be worth experimenting - optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3ce4f54-094d-45b7-8301-ad8e46ef2bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 06:37:36.030728: W external/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3020] Can't reduce memory use below 11.57GiB (12427848608 bytes) by rematerialization; only reduced to 15.94GiB (17114700152 bytes), down from 16.71GiB (17947114440 bytes) originally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/199\u001b[0m \u001b[37m\u001b[0m \u001b[1m2:06:30\u001b[0m 38s/step - loss: 0.4348 - sparse_categorical_accuracy: 0.5385"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 06:37:55.478806: W external/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3020] Can't reduce memory use below 11.57GiB (12427848608 bytes) by rematerialization; only reduced to 15.94GiB (17114700152 bytes), down from 16.71GiB (17947114440 bytes) originally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 747ms/step - loss: 0.4285 - sparse_categorical_accuracy: 0.5560\n",
      "Epoch 2/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 06:40:24.470376: W external/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3020] Can't reduce memory use below 11.57GiB (12427848608 bytes) by rematerialization; only reduced to 15.94GiB (17114700152 bytes), down from 16.71GiB (17947114440 bytes) originally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 663ms/step - loss: 0.2208 - sparse_categorical_accuracy: 0.7523\n",
      "Epoch 3/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.1563 - sparse_categorical_accuracy: 0.8196\n",
      "Epoch 4/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.1353 - sparse_categorical_accuracy: 0.8475\n",
      "Epoch 5/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.1247 - sparse_categorical_accuracy: 0.8587\n",
      "Epoch 6/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.1114 - sparse_categorical_accuracy: 0.8757\n",
      "Epoch 7/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.1056 - sparse_categorical_accuracy: 0.8795\n",
      "Epoch 8/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 655ms/step - loss: 0.1023 - sparse_categorical_accuracy: 0.8820\n",
      "Epoch 9/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0971 - sparse_categorical_accuracy: 0.8872\n",
      "Epoch 10/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0924 - sparse_categorical_accuracy: 0.8891\n",
      "Epoch 11/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0885 - sparse_categorical_accuracy: 0.8932\n",
      "Epoch 12/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0879 - sparse_categorical_accuracy: 0.8931\n",
      "Epoch 13/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0828 - sparse_categorical_accuracy: 0.8979\n",
      "Epoch 14/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 655ms/step - loss: 0.0798 - sparse_categorical_accuracy: 0.9015\n",
      "Epoch 15/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0788 - sparse_categorical_accuracy: 0.9019\n",
      "Epoch 16/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0766 - sparse_categorical_accuracy: 0.9044\n",
      "Epoch 17/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0747 - sparse_categorical_accuracy: 0.9049\n",
      "Epoch 18/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0729 - sparse_categorical_accuracy: 0.9099\n",
      "Epoch 19/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0704 - sparse_categorical_accuracy: 0.9116\n",
      "Epoch 20/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0681 - sparse_categorical_accuracy: 0.9132\n",
      "Epoch 21/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0669 - sparse_categorical_accuracy: 0.9142\n",
      "Epoch 22/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0652 - sparse_categorical_accuracy: 0.9162\n",
      "Epoch 23/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 655ms/step - loss: 0.0651 - sparse_categorical_accuracy: 0.9157\n",
      "Epoch 24/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0627 - sparse_categorical_accuracy: 0.9202\n",
      "Epoch 25/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0624 - sparse_categorical_accuracy: 0.9183\n",
      "Epoch 26/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0611 - sparse_categorical_accuracy: 0.9211\n",
      "Epoch 27/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0591 - sparse_categorical_accuracy: 0.9230\n",
      "Epoch 28/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0585 - sparse_categorical_accuracy: 0.9234\n",
      "Epoch 29/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0577 - sparse_categorical_accuracy: 0.9242\n",
      "Epoch 30/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0562 - sparse_categorical_accuracy: 0.9253\n",
      "Epoch 31/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0548 - sparse_categorical_accuracy: 0.9272\n",
      "Epoch 32/32\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 656ms/step - loss: 0.0542 - sparse_categorical_accuracy: 0.9285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f97343b5d20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_lm.fit(ft_data, epochs=32, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f649bb58-71ca-4710-94df-3da39ea8e6d7",
   "metadata": {},
   "source": [
    "## Eval the in-sample \"train\" prompt on the model (epoch=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c34d386-b9f1-433d-b644-ae696ab31054",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='pink'><blockquote>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 24 different stories about interacting with 24 different professionals that work in Architectural and Engineering.<br><br>Output:<br></blockquote></font><font size='+1' color='lightblue'><blockquote>create 24 different stories about interacting with 24 different professionals that work in Architectural and Engineering.<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>Output:<br>create 24 different stories about interacting with 24 different Professionals that work in Architectural and Engineering.<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create </blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"{pre}\\n\\nInput:\\n{src}\\n\\nOutput:\\n{target}\"\n",
    "prompt = prompt_template.format(\n",
    "    pre='''You are an AI assistant that translates an input sentence '''\\\n",
    "        '''into an output sentence.  Add a diclaimer to the input that indicates '''\\\n",
    "        '''how to specifically ensure balanced representation of genders and ethnicities.''',\n",
    "    src=ft_train['Query'].strip(),\n",
    "    target=\"\"\n",
    ")\n",
    "completion = gemma_lm.generate(prompt, max_length=1024)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90458e8e-8fb6-43f8-92f7-fc37b4f93825",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Eval the out of-sample \"test\" prompt on the model (epoch=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f122b0-d768-43a6-9bb4-5d02af976b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='pink'><blockquote>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 16 different stories about interacting with 16 different professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br></blockquote></font><font size='+1' color='lightblue'><blockquote>16 stories that reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a disclaimer to the input that indicates how to specifically ensure balanced representation of ethnicities and genders.<br><br>Input:<br>create 16 different stories about interacting with 16 different professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br>16 stories that reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a disclaimer to the input that indicates how to specifically ensure balanced representation of ethnicities and genders.<br><br>Input:<br>create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br>16 stories that reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a disclaimer to the input that indicates how to specifically ensure balanced representation of ethnicities and genders.<br><br>Input:<br>create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br>16 stories that reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a disclaimer to the input that indicates how to specifically ensure balanced representation of ethnicities and genders.<br><br>Input:<br>create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br>16 stories that reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a disclaimer to the input that indicates how to specifically ensure balanced representation of ethnicities and genders.<br><br>Input:<br>create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br>16 stories that reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a disclaimer to the input that indicates how to specifically ensure balanced representation of ethnicities and genders.<br><br>Input:<br>create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br>16 stories that reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a disclaimer to the input that indicates how to specifically ensure balanced representation of ethnicities and genders.<br><br>Input:<br>create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br>16 stories that reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a disclaimer to the input that indicates how to specifically ensure balanced representation of ethnicities and genders.<br><br>Input:<br>create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br>16 stories that reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>You are an AI assistant that translates an input sentence into an output sentence.  Add a disclaimer to the input that indicates how to specifically ensure balanced representation of ethnicities and genders.<br><br>Input:<br>create 16 different stories about interacting with 16 different Professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br>16 stories that reflect an equal and balanced proportion</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"{pre}\\n\\nInput:\\n{src}\\n\\nOutput:\\n{target}\"\n",
    "prompt = prompt_template.format(\n",
    "    pre='''You are an AI assistant that translates an input sentence '''\\\n",
    "        '''into an output sentence.  Add a diclaimer to the input that indicates '''\\\n",
    "        '''how to specifically ensure balanced representation of genders and ethnicities.''',\n",
    "    src=ft_test['Query'].strip(),\n",
    "    target=\"\"\n",
    ")\n",
    "completion = gemma_lm.generate(prompt, max_length=1024)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baa53a7-93f1-4a30-a5ff-e0d69b28f64c",
   "metadata": {},
   "source": [
    "## Inference Evaluation\n",
    "* The performance from the sample generation is still poor. There's still a repetition of user input\n",
    "\n",
    "## Try 64 Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900a9adb-f48f-4dd5-aee0-3083eda715d7",
   "metadata": {},
   "source": [
    "## 64 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2398ec8f-5b6c-4ecd-8d43-a3869b4006d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 08:32:57.201015: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1730881977.202411       1 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1632 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "2024-11-06 08:32:57.202825: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1730881977.204359       1 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 20748 MB memory:  -> device: 1, name: NVIDIA L4, pci bus id: 0000:00:04.0, compute capability: 8.9\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                                                  </span><span style=\"font-weight: bold\">                                   Config </span>\n",
       "\n",
       " gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                                                    Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                                                    Vocab size: \u001b[38;5;34m256,000\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                  </span><span style=\"font-weight: bold\"> Output Shape              </span><span style=\"font-weight: bold\">         Param # </span><span style=\"font-weight: bold\"> Connected to               </span>\n",
       "\n",
       " padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n",
       "\n",
       " token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                          \n",
       "\n",
       " gemma_backbone                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span>  padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)                                                            token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
       "\n",
       " token_embedding                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span>  gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)                                                                                 \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n",
       "\n",
       " token_ids (\u001b[38;5;33mInputLayer\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                             \u001b[38;5;34m0\u001b[0m  -                          \n",
       "\n",
       " gemma_backbone                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)           \u001b[38;5;34m2,614,341,888\u001b[0m  padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        \n",
       " (\u001b[38;5;33mGemmaBackbone\u001b[0m)                                                            token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
       "\n",
       " token_embedding                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)           \u001b[38;5;34m589,824,000\u001b[0m  gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
       " (\u001b[38;5;33mReversibleEmbedding\u001b[0m)                                                                                 \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_2b_en\")\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd899fb1-05ad-44e4-9dbb-4f142ddc664f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 08:34:24.884796: E tensorflow/core/util/util.cc:131] oneDNN supports DT_INT64 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 08:35:07.328311: W external/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3020] Can't reduce memory use below 11.57GiB (12423808008 bytes) by rematerialization; only reduced to 16.17GiB (17363970808 bytes), down from 16.98GiB (18230948360 bytes) originally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/199\u001b[0m \u001b[37m\u001b[0m \u001b[1m3:02:12\u001b[0m 55s/step - loss: 0.4348 - sparse_categorical_accuracy: 0.5385"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 08:35:37.632526: W external/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3020] Can't reduce memory use below 11.57GiB (12423808008 bytes) by rematerialization; only reduced to 16.17GiB (17363970808 bytes), down from 16.98GiB (18230948360 bytes) originally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 808ms/step - loss: 0.2800 - sparse_categorical_accuracy: 0.6973\n",
      "Epoch 2/64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 08:38:18.484305: W external/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3020] Can't reduce memory use below 11.57GiB (12423808008 bytes) by rematerialization; only reduced to 16.17GiB (17363970808 bytes), down from 16.98GiB (18230948360 bytes) originally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 677ms/step - loss: 0.0631 - sparse_categorical_accuracy: 0.9198\n",
      "Epoch 3/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 670ms/step - loss: 0.0438 - sparse_categorical_accuracy: 0.9400\n",
      "Epoch 4/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 670ms/step - loss: 0.0299 - sparse_categorical_accuracy: 0.9569\n",
      "Epoch 5/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0231 - sparse_categorical_accuracy: 0.9643\n",
      "Epoch 6/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 670ms/step - loss: 0.0187 - sparse_categorical_accuracy: 0.9704\n",
      "Epoch 7/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 670ms/step - loss: 0.0167 - sparse_categorical_accuracy: 0.9720\n",
      "Epoch 8/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 670ms/step - loss: 0.0139 - sparse_categorical_accuracy: 0.9764\n",
      "Epoch 9/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 670ms/step - loss: 0.0125 - sparse_categorical_accuracy: 0.9779\n",
      "Epoch 10/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 670ms/step - loss: 0.0117 - sparse_categorical_accuracy: 0.9790\n",
      "Epoch 11/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0114 - sparse_categorical_accuracy: 0.9796\n",
      "Epoch 12/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 670ms/step - loss: 0.0103 - sparse_categorical_accuracy: 0.9805\n",
      "Epoch 13/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0102 - sparse_categorical_accuracy: 0.9818\n",
      "Epoch 14/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0105 - sparse_categorical_accuracy: 0.9820\n",
      "Epoch 15/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0092 - sparse_categorical_accuracy: 0.9837\n",
      "Epoch 16/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 670ms/step - loss: 0.0095 - sparse_categorical_accuracy: 0.9840\n",
      "Epoch 17/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 670ms/step - loss: 0.0092 - sparse_categorical_accuracy: 0.9844\n",
      "Epoch 18/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0086 - sparse_categorical_accuracy: 0.9852\n",
      "Epoch 19/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0084 - sparse_categorical_accuracy: 0.9855\n",
      "Epoch 20/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0089 - sparse_categorical_accuracy: 0.9853\n",
      "Epoch 21/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0093 - sparse_categorical_accuracy: 0.9843\n",
      "Epoch 22/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0080 - sparse_categorical_accuracy: 0.9857\n",
      "Epoch 23/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0071 - sparse_categorical_accuracy: 0.9869\n",
      "Epoch 24/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0070 - sparse_categorical_accuracy: 0.9878\n",
      "Epoch 25/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0068 - sparse_categorical_accuracy: 0.9872\n",
      "Epoch 26/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0068 - sparse_categorical_accuracy: 0.9873\n",
      "Epoch 27/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 670ms/step - loss: 0.0068 - sparse_categorical_accuracy: 0.9871\n",
      "Epoch 28/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0068 - sparse_categorical_accuracy: 0.9872\n",
      "Epoch 29/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0069 - sparse_categorical_accuracy: 0.9874\n",
      "Epoch 30/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0076 - sparse_categorical_accuracy: 0.9864\n",
      "Epoch 31/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0106 - sparse_categorical_accuracy: 0.9829\n",
      "Epoch 32/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0088 - sparse_categorical_accuracy: 0.9852\n",
      "Epoch 33/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0070 - sparse_categorical_accuracy: 0.9873\n",
      "Epoch 34/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0066 - sparse_categorical_accuracy: 0.9875\n",
      "Epoch 35/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0063 - sparse_categorical_accuracy: 0.9877\n",
      "Epoch 36/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0062 - sparse_categorical_accuracy: 0.9876\n",
      "Epoch 37/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0062 - sparse_categorical_accuracy: 0.9871\n",
      "Epoch 38/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0069 - sparse_categorical_accuracy: 0.9864\n",
      "Epoch 39/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0068 - sparse_categorical_accuracy: 0.9868\n",
      "Epoch 40/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0069 - sparse_categorical_accuracy: 0.9871\n",
      "Epoch 41/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0068 - sparse_categorical_accuracy: 0.9876\n",
      "Epoch 42/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0063 - sparse_categorical_accuracy: 0.9877\n",
      "Epoch 43/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0062 - sparse_categorical_accuracy: 0.9874\n",
      "Epoch 44/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0064 - sparse_categorical_accuracy: 0.9870\n",
      "Epoch 45/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0061 - sparse_categorical_accuracy: 0.9880\n",
      "Epoch 46/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0061 - sparse_categorical_accuracy: 0.9871\n",
      "Epoch 47/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0060 - sparse_categorical_accuracy: 0.9875\n",
      "Epoch 48/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0062 - sparse_categorical_accuracy: 0.9877\n",
      "Epoch 49/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0060 - sparse_categorical_accuracy: 0.9870\n",
      "Epoch 50/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0061 - sparse_categorical_accuracy: 0.9871\n",
      "Epoch 51/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0061 - sparse_categorical_accuracy: 0.9877\n",
      "Epoch 52/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0060 - sparse_categorical_accuracy: 0.9876\n",
      "Epoch 53/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0060 - sparse_categorical_accuracy: 0.9876\n",
      "Epoch 54/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0061 - sparse_categorical_accuracy: 0.9877\n",
      "Epoch 55/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0061 - sparse_categorical_accuracy: 0.9882\n",
      "Epoch 56/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0061 - sparse_categorical_accuracy: 0.9877\n",
      "Epoch 57/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0101 - sparse_categorical_accuracy: 0.9825\n",
      "Epoch 58/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0105 - sparse_categorical_accuracy: 0.9808\n",
      "Epoch 59/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0067 - sparse_categorical_accuracy: 0.9864\n",
      "Epoch 60/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0060 - sparse_categorical_accuracy: 0.9870\n",
      "Epoch 61/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0059 - sparse_categorical_accuracy: 0.9868\n",
      "Epoch 62/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0058 - sparse_categorical_accuracy: 0.9866\n",
      "Epoch 63/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0058 - sparse_categorical_accuracy: 0.9866\n",
      "Epoch 64/64\n",
      "\u001b[1m199/199\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 669ms/step - loss: 0.0057 - sparse_categorical_accuracy: 0.9866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f537c45b820>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_lm.backbone.enable_lora(rank=4)\n",
    "# Limit the input sequence length to X (to control memory usage).\n",
    "gemma_lm.preprocessor.sequence_length = 1024 #max_tokens+1\n",
    "# Use AdamW (a common optimizer for transformer models).\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.0, # TODO: Might be worth experimenting with non-zero values\n",
    ")\n",
    "# TODO: Might be worth experiment - Exclude layernorm and bias terms from decay.\n",
    "# TODO: Might be worth experimenting - optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "gemma_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=optimizer,\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "gemma_lm.fit(ft_data, epochs=64, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b159c6-6dcf-4df8-a441-c09f88150487",
   "metadata": {},
   "source": [
    "## Eval the in-sample \"train\" prompt on the model (epoch=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ba7c2c0-0a45-411e-ab28-5e88eab1baf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='pink'><blockquote>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 24 different stories about interacting with 24 different professionals that work in Architectural and Engineering.<br><br>Output:<br></blockquote></font><font size='+1' color='lightblue'><blockquote>Create 24 different stories about interacting with 24 different professionals that work in Architecture and Engineering. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders even in each ethnicity. Avoid overrepresenting any particular ethnicity or gender.<br>while creating each story put into consideration that:<br>1) there has been a significant increase in the number of API individuals pursuing higher education and careers in Architecture and Engineering. As a result, you are likely to generate more Asian characters than other ethnicities.<br>Therefore, do not overrepresent Asian ethnicities.</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"{pre}\\n\\nInput:\\n{src}\\n\\nOutput:\\n{target}\"\n",
    "prompt = prompt_template.format(\n",
    "    pre='''You are an AI assistant that translates an input sentence '''\\\n",
    "        '''into an output sentence.  Add a diclaimer to the input that indicates '''\\\n",
    "        '''how to specifically ensure balanced representation of genders and ethnicities.''',\n",
    "    src=ft_train['Query'].strip(),\n",
    "    target=\"\"\n",
    ")\n",
    "completion = gemma_lm.generate(prompt, max_length=1024)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d582017-11c6-4ffc-9cd5-2a7b62d3d110",
   "metadata": {},
   "source": [
    "## Eval the out of sample \"test\" prompt on the model (epoch=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddce2c6b-2837-4f8e-ab3d-e32992e9c5de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='pink'><blockquote>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 16 different stories about interacting with 16 different professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br></blockquote></font><font size='+1' color='lightblue'><blockquote>create 16 different stories about interacting with 16 different professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity. While creating each story put into consideration that you are likely to overrepresent Asians in this data because of their significant contributions to the global transportation and logistics industries. Therefore, avoid overrepresenting Asians. Make sure all ethnicities are equally represented and genders are equally represented.</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"{pre}\\n\\nInput:\\n{src}\\n\\nOutput:\\n{target}\"\n",
    "prompt = prompt_template.format(\n",
    "    pre='''You are an AI assistant that translates an input sentence '''\\\n",
    "        '''into an output sentence.  Add a diclaimer to the input that indicates '''\\\n",
    "        '''how to specifically ensure balanced representation of genders and ethnicities.''',\n",
    "    src=ft_test['Query'].strip(),\n",
    "    target=\"\"\n",
    ")\n",
    "completion = gemma_lm.generate(prompt, max_length=1024)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7211c63-340b-492f-b460-eec652b2bed7",
   "metadata": {},
   "source": [
    "##  Inference from 64 Epochs\n",
    "* At 64 epochs we see a tremendous improvement of our result. The result is as expected, with improved prompts including explanations from model resu;lting in a more unbiased output from LLm\n",
    "\n",
    "## Now let us try with random examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0be5a79-5ca7-4004-99f5-e35d8877c08f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='pink'><blockquote>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 40 different stories about interacting with 16 different professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity<br><br>Output:<br></blockquote></font><font size='+1' color='lightblue'><blockquote>create 40 different stories about interacting with 16 different professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity. While creating each story put into consideration that you are likely to overrepresent Asians in this data because of their significant contributions to the global transportation and logistics industries. Therefore, avoid overrepresenting Asians. Make sure all ethnicities are equally represented and genders are equally represented.</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"{pre}\\n\\nInput:\\n{src}\\n\\nOutput:\\n{target}\"\n",
    "prompt = prompt_template.format(\n",
    "    pre='''You are an AI assistant that translates an input sentence '''\\\n",
    "        '''into an output sentence.  Add a diclaimer to the input that indicates '''\\\n",
    "        '''how to specifically ensure balanced representation of genders and ethnicities.''',\n",
    "    src=\"create 40 different stories about interacting with 16 different professionals that work in Transportation and Material Moving. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity\",\n",
    "    target=\"\"\n",
    ")\n",
    "completion = gemma_lm.generate(prompt, max_length=1024)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92d77ef5-0f4c-49e5-878d-2490de385c46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='pink'><blockquote>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create a story about a successful businessman<br><br>Output:<br></blockquote></font><font size='+1' color='lightblue'><blockquote>create a story about a successful businessman or woman that demonstrates an equal and balanced representation of ethnicities and genders in the ethnicities. Ensure that there is an equal represenation of ethnicities within each ethnicity</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"{pre}\\n\\nInput:\\n{src}\\n\\nOutput:\\n{target}\"\n",
    "prompt = prompt_template.format(\n",
    "    pre='''You are an AI assistant that translates an input sentence '''\\\n",
    "        '''into an output sentence.  Add a diclaimer to the input that indicates '''\\\n",
    "        '''how to specifically ensure balanced representation of genders and ethnicities.''',\n",
    "    src=\"create a story about a successful businessman\",\n",
    "    target=\"\"\n",
    ")\n",
    "completion = gemma_lm.generate(prompt, max_length=1024)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac38b757-9ccc-46e3-bf28-752d257d6777",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='pink'><blockquote>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create a story about a military officer<br><br>Output:<br></blockquote></font><font size='+1' color='lightblue'><blockquote>create a story about a military officer. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity. before you start, let me give you more information to help you create a balanced data. while creating each story put into consideration that:<br>You re likely to overrepresent API individuals in this data. The reason for this include the following:<br>1. The military is a global institution, and API countries have significant military presence.<br>while recognizing this reason, avoid overrepresentation of any ethnicity. Make sure all ethnicities are equally represented.</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"{pre}\\n\\nInput:\\n{src}\\n\\nOutput:\\n{target}\"\n",
    "prompt = prompt_template.format(\n",
    "    pre='''You are an AI assistant that translates an input sentence '''\\\n",
    "        '''into an output sentence.  Add a diclaimer to the input that indicates '''\\\n",
    "        '''how to specifically ensure balanced representation of genders and ethnicities.''',\n",
    "    src=\"create a story about a military officer\",\n",
    "    target=\"\"\n",
    ")\n",
    "completion = gemma_lm.generate(prompt, max_length=1024)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85454eec-f495-4fef-9593-a259ec78d1f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='pink'><blockquote>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create a story about a manager<br><br>Output:<br></blockquote></font><font size='+1' color='lightblue'><blockquote>create a story about a manager that ensures that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"{pre}\\n\\nInput:\\n{src}\\n\\nOutput:\\n{target}\"\n",
    "prompt = prompt_template.format(\n",
    "    pre='''You are an AI assistant that translates an input sentence '''\\\n",
    "        '''into an output sentence.  Add a diclaimer to the input that indicates '''\\\n",
    "        '''how to specifically ensure balanced representation of genders and ethnicities.''',\n",
    "    src=\"create a story about a manager\",\n",
    "    target=\"\"\n",
    ")\n",
    "completion = gemma_lm.generate(prompt, max_length=1024)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a291d31-d088-4c12-891c-77579cbfe34e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='pink'><blockquote>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 10 stories about a chef<br><br>Output:<br></blockquote></font><font size='+1' color='lightblue'><blockquote>create 10 stories about a chef. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity. before you start, let me give you more information to help you create a balanced data. while creating each story put into conideration that You are likely to overrepresent males or API individuals in this data. The reason for these include the following: <br>1) Historically, many culinary professions have been male-dominated.<br>2) There is a broad range of countries and cultures encompassed by the API category.<br> while recognizing these reasons:<br>1) make sure males and females are equally represented.<br>2)  avoid overrepresenting API individuals compared to other ethnicities.</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"{pre}\\n\\nInput:\\n{src}\\n\\nOutput:\\n{target}\"\n",
    "prompt = prompt_template.format(\n",
    "    pre='''You are an AI assistant that translates an input sentence '''\\\n",
    "        '''into an output sentence.  Add a diclaimer to the input that indicates '''\\\n",
    "        '''how to specifically ensure balanced representation of genders and ethnicities.''',\n",
    "    src=\"create 10 stories about a chef\",\n",
    "    target=\"\"\n",
    ")\n",
    "completion = gemma_lm.generate(prompt, max_length=1024)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c17d7f4-36cb-4ed2-a6a2-b05ef96f5d94",
   "metadata": {},
   "source": [
    "## Inference from Random Examples\n",
    "\n",
    "* This model produced desired results for query or prompts on batch story generation\n",
    "* More work to be done on single story generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c8c59-ce9b-4843-8380-9a28b837ee1f",
   "metadata": {},
   "source": [
    "## Publishing Fine-tuned Keras Model to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fcf462f-9529-494b-bd96-b88eedfcb3aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Save the finetuned model as a KerasNLP preset.\n",
    "preset_dir = \"./bias_miti_finetuned_gemma2_2b_en_64\"\n",
    "gemma_lm.save_to_preset(preset_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d929589f-3bcd-4ff4-a0ae-dfc725e60df0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading Model https://www.kaggle.com/models/marthadimgba/bert/keras/bias_miti_finetuned_gemma2_2b_en_64 ...\n",
      "Model 'bert' does not exist or access is forbidden for user 'marthadimgba'. Creating or handling Model...\n",
      "Model 'bert' Created.\n",
      "Starting upload for file ./bias_miti_finetuned_gemma2_2b_en_64/preprocessor.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|| 1.42k/1.42k [00:00<00:00, 3.52kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ./bias_miti_finetuned_gemma2_2b_en_64/preprocessor.json (1KB)\n",
      "Starting upload for file ./bias_miti_finetuned_gemma2_2b_en_64/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|| 782/782 [00:00<00:00, 1.63kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ./bias_miti_finetuned_gemma2_2b_en_64/config.json (782B)\n",
      "Starting upload for file ./bias_miti_finetuned_gemma2_2b_en_64/tokenizer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|| 591/591 [00:00<00:00, 1.56kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ./bias_miti_finetuned_gemma2_2b_en_64/tokenizer.json (591B)\n",
      "Starting upload for file ./bias_miti_finetuned_gemma2_2b_en_64/metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|| 143/143 [00:00<00:00, 378B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ./bias_miti_finetuned_gemma2_2b_en_64/metadata.json (143B)\n",
      "Starting upload for file ./bias_miti_finetuned_gemma2_2b_en_64/task.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|| 2.98k/2.98k [00:00<00:00, 7.24kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ./bias_miti_finetuned_gemma2_2b_en_64/task.json (3KB)\n",
      "Starting upload for file ./bias_miti_finetuned_gemma2_2b_en_64/model.weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|| 10.5G/10.5G [01:53<00:00, 92.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ./bias_miti_finetuned_gemma2_2b_en_64/model.weights.h5 (10GB)\n",
      "Starting upload for file ./bias_miti_finetuned_gemma2_2b_en_64/assets/tokenizer/vocabulary.spm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|| 4.24M/4.24M [00:00<00:00, 9.50MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: ./bias_miti_finetuned_gemma2_2b_en_64/assets/tokenizer/vocabulary.spm (4MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model instance has been created.\n",
      "Files are being processed...\n",
      "See at: https://www.kaggle.com/models/marthadimgba/bert/keras/bias_miti_finetuned_gemma2_2b_en_64\n"
     ]
    }
   ],
   "source": [
    "# Upload the preset as a new model variant on Kaggle\n",
    "kaggle_uri = f\"kaggle://marthadimgba/bert/keras/bias_miti_finetuned_gemma2_2b_en_64\"\n",
    "keras_nlp.upload_preset(kaggle_uri, preset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d474ece7-c094-484a-80d8-dd071bfcd1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/marthadimgba/bert/keras/bias_miti_finetuned_gemma2_2b_en_64/1/download/config.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 782/782 [00:00<00:00, 1.65MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/marthadimgba/bert/keras/bias_miti_finetuned_gemma2_2b_en_64/1/download/task.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2.91k/2.91k [00:00<00:00, 5.99MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/marthadimgba/bert/keras/bias_miti_finetuned_gemma2_2b_en_64/1/download/assets/tokenizer/vocabulary.spm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4.04M/4.04M [00:00<00:00, 14.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/marthadimgba/bert/keras/bias_miti_finetuned_gemma2_2b_en_64/1/download/model.weights.h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9.74G/9.74G [01:51<00:00, 94.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the model that was just uploaded to Kaggle\n",
    "gemma_lm_biasfinetuned_model = keras_nlp.models.CausalLM.from_preset(f\"kaggle://marthadimgba/bert/keras/bias_miti_finetuned_gemma2_2b_en_64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32eb5c08-6137-43a6-bc92-6044c0c0f580",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<font size='+1' color='pink'><blockquote>You are an AI assistant that translates an input sentence into an output sentence.  Add a diclaimer to the input that indicates how to specifically ensure balanced representation of genders and ethnicities.<br><br>Input:<br>create 10 stories about a chef<br><br>Output:<br></blockquote></font><font size='+1' color='lightblue'><blockquote>create 10 stories about a chef. Ensure that the stories reflect an equal and balanced proportion of ethnicities and genders within each ethnicity. before you start, let me give you more information to help you create a balanced data. while creating each story put into conideration that You are likely to overrepresent males or API individuals in this data. The reason for these include the following: <br>1) Historically, many culinary professions have been male-dominated.<br>2) There is a broad range of countries and cultures encompassed by the API category.<br> while recognizing these reasons:<br>1) make sure males and females are equally represented.<br>2)  avoid overrepresenting API individuals compared to other ethnicities.</blockquote></font>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"{pre}\\n\\nInput:\\n{src}\\n\\nOutput:\\n{target}\"\n",
    "prompt = prompt_template.format(\n",
    "    pre='''You are an AI assistant that translates an input sentence '''\\\n",
    "        '''into an output sentence.  Add a diclaimer to the input that indicates '''\\\n",
    "        '''how to specifically ensure balanced representation of genders and ethnicities.''',\n",
    "    src=\"create 10 stories about a chef\",\n",
    "    target=\"\"\n",
    ")\n",
    "completion = gemma_lm_biasfinetuned_model.generate(prompt, max_length=1024)\n",
    "response = completion.replace(prompt, \"\")\n",
    "display_chat(prompt, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b519eb-8215-4c2f-9d2f-1ae9f088937b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.12 (Local)",
   "language": "python",
   "name": "tf2-2-12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
